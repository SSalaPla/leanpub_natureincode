#3. Genetic Drift

In the previous chapter, we have established that under Hardy-Weinberg assumptions - infinite population size, random mating, no mutation or selection, etc. - nothing ever changes. Allele frequencies stay the same forever, and if the genotype frequencies are not currently at Hardy-Weinberg frequencies, they will get there in a single generation, and then remain there forever.

It’s now time to relax some if these assumptions. The first assumption that we are going to relax is that of the infinite population size. We are now going to assume that populations are finite. It turns out that this has enormous effects for evolution. When populations are finite, chance effects will start to play a role. These chance effects are stronger when populations are smaller. It is intuitively easy to understand this. Suppose you toss a coin - you know that on average, it will come up heads half of the time, and tails the other half of the time. However, if you only toss the coin a few times, you can get very non-even distributions of heads and tails. Say you toss in ten times. It may end up tossing 5 heads and 5 tails (5:5), but you will often find yourself tossing 6:4, or 7:3. Sometimes you’ll even get 8:2 or 9:1; and even a 10:0, although very rare, wouldn’t be totally unexpected. If you now increase your coin tossing efforts by one order of magnitude, and toss your coin 100 times, you would notice that it’s very rare to get beyond a 30:70 ratio, and you would hardly ever hit 20:80 or below. And in fact, getting tails or heads 100 times in a row is so unlikely, it's hard to come up with a comparison. The best I can come up with is that it's about as likely as winning the lottery four consecutive times. Increase your coin tossing efforts again by a order of magnitude (1000 tosses), and anything less equal than 450:550 becomes highly improbable.

You get the idea: The more often you toss the coin, the closer you will get to the perfect equilibrium that you would expect. The implication of this is that *small population sizes will be more prone to random chance effects than large populations*. However, all finite population sizes are prone to chance effects, and over evolutionary timescales, even large population sizes where random chance effects are weak in the short term may eventually show signs of these effects. When allele frequencies change over time (i.e. when evolution occurs) due to these random effects, we call it *genetic drift*.

This is an important chapter. It’s important because it introduces the concept of randomness, a key concept in all of biology. One could even argue that the main difference between the dynamic of living systems (biological systems), and the the dynamics of non-living systems (chemical and physical systems) is randomness. The science of evolutionary biology has been dominated for a long time by the perspective of natural selection. Nowadays, we understand that natural selection is not the only process affecting evolution. Which force is more important for evolution - natural selection or genetic drift - is one of the great debates in contemporary evolutionary biology. But even those favoring natural selection would agree that randomness, and genetic drift, is a major force. In this chapter, we will shed some light on this force, and on its consequences.


##Randomness

Randomness is the idea of events occurring at random, by chance. If they happen by chance, it’s impossible to predict them. The more sophisticated term for randomness is stochasticity - random events are said to be *stochastic*. 

While random events cannot be predicted, we can still say something about their probability of occurring. This is the main idea behind the theory of probability. For example, a fair coin comes up heads half of the time, and tails the other half of the time. While we can’t predict the outcome of a single coin toss, we still know that the chance, or probability, that the coin comes up heads, is exactly 50% (in reality, no coin is exactly fair, and the probability of heads is probably slightly lower or higher than 50%, but in a fair coin, the probability is 50% by definition). Because we know that each coin toss comes up 50% heads and 50% tails, we can calculate the probability that ten coin tosses come up heads 80% of time, and tails 20% of the time, for example. For any ten coin tosses, it is completely impossible to predict exactly how many will come up heads; but because we can calculate the probability of any outcome, we can at least get a good sense of what to expect. For example, we can calculate that the probability of 80% heads 20% tails is 4.39% like so: {$$}\frac{\frac{10*9}{2}}{2^{10}} = 0.0439{/$$} (don’t worry about how to calculate this, but if you are interested, here is a good explanation: [http://gwydir.demon.co.uk/jo/probability/info.htm](http://gwydir.demon.co.uk/jo/probability/info.htm). An outcome like this isn’t unlikely, but you wouldn’t want to bet the farm on it either.

Before we get to the biology of randomness, I want to jump straight to code. The great thing about having fast computers is that we can simulate randomness over and over again, in a very short amount of time. This allows us to run millions of coin tossing experiments in a computer in a split second.

In JavaScript, the quintessential method to produce randomness is `Math.random()`. It takes no arguments, but produces a random number between 0 and 1. Let’s try it out. Create a new html document with a `<script>` element, and log the output of the method like so:

~~~~~~~~
console.log(Math.random());
~~~~~~~~

Save and load the file in the browser, and take a look at the console output. Indeed, a random number between 0 and 1 has been generated. Verify this by reloading the page over and over again. You’ll notice that the number is different, every time you reload the page, as it should be. 

Let’s go ahead and verify the claim I made. I claimed that `Math.random()` produces a random number between 0 and 1. I'm going to clarify this further by saying not only will it generate a random number between 0 and 1, but that each number is equally likely to be generated. If that is true, then the average output of `Math.random()` should be 0.5. To verify that, let’s use a `for` loop to create many random numbers, and then calculate the average of these numbers:

~~~~~~~~
var sum = 0;
var repeats = 100;
for (var i=0; i<repeats; i=i+1) {
    sum = sum + Math.random();
}
average = sum / repeats;
console.log("The average is", average);
~~~~~~~~

If you followed the examples in the previous chapters, this should now look familiar. At first, we are declaring two variables, `sum` and `repeats`. Then, we’re using a `for` loop to add to `sum` whatever `Math.random()` returns, summing up the random numbers. We do this exactly as many times as defined by the variable `repeats`. Then, we simply calculate and print the average.

When I do this, I’m getting the and average of 

~~~~~~~~
0.533668438885361
~~~~~~~~

Upon reload, I’m getting

~~~~~~~~
0.46173790065106
~~~~~~~~

Another reload gives me 

~~~~~~~~
0.4819509003055282 
~~~~~~~~

and so on. This isn’t very precise, so let’s maybe increase the number of repeats. You should be able to increase repeats to 100,000 without having your browser breaking a sweat. With 100,000 repeats, I’m now getting

~~~~~~~~
0.5000435156048741
0.5006688627070328 
0.4989869923099107 
~~~~~~~~

when I reload the document three times.Already much closer! 

Try increasing your repeats by a factor of ten, for as long as your browser can handle it (stop when it takes a few seconds). On my laptop, I can go up to 1 billion repeats before I have to wait a few seconds, and then I’m getting

~~~~~~~~
0.499998771590113
0.49996582762364633
0.5000288064449279 
~~~~~~~~

As you can see, the more repeats, the closer we get to 0.5. However, keep in mind that floating point numbers have a limited accuracy, as discussed in the previous chapter. Usually, you won’t ever notice this, because the inaccuracy occurs at insignificant digits, but if you keep adding numbers, for example, the inaccuracy can potentially add up too. The point of this exercise was not to lead you astray with floating point accuracy, which you will probably never encounter as a problem. The point was to introduce you to `Math.random()` - and to demonstrate how powerful our computers are nowadays. In just a few seconds, my laptop computer generated a billion numbers and added them up!

Now let’s go back to the previous coin tossing example. I mentioned that if I tossed a fair coin ten times, the probability of 80% heads and 20% tails is 4.39%. How can we verify this in JavaScript? By actually running the coin-tossing experiment in the computer! This is a so-called *simulation*, where you simulate a process from the real world in the virtual world of a computer. 

First, let’s implement the coin tossing. Create a new HTML file and have it execute the following script:

~~~~~~~~
var throws = 10;
var heads = 0;
var tails = 0;
for (var i=0; i<throws; i=i+1) {
    if (Math.random() <= 0.5) {
        heads = heads + 1;
    }
    else {     
        tails = tails + 1;   
    }
}
console.log(heads,"heads",tails,"tails");
~~~~~~~~

This should give you an output like 4 "heads" 6 "tails", and on reload, the output should change (although not always - by chance you might get the same outcome two times in a row).

Let’s go through this code in detail, because it introduces a new concept we haven’t met yet: that of control flow. The code setup is straightforward - you initiate three variables, and then implement a `for` loop throwing the coins using `Math.random()`. The variables `heads` and `tails` should act as counters, so that whenever the coin comes up heads (i.e. when `Math.random()` returns a value that is smaller or equal to `0.5`), we increase the `heads` counter by one, and whenever the coin comes up tails (i.e. when `Math.random()` returns a value that is greater than `0.5`), we increase the `tails` counter by one.

In other words, the execution of some of the code (e.g. increase `heads` counter by one) is *conditional* - the code should only be executed if a given condition is met. As you can imagine, this is a key concept in programming. The way this concept can be implemented in JavaScript is as follows:

~~~~~~~~
if (condition) {
    statement1
}
else {
    statement2
}
~~~~~~~~

This is easy to read: if the `condition` is true, then execute `statement1`, otherwise execute `statement2`. Sometimes, the otherwise statement is simply to do nothing, in which case you can omit the `else` altogether and simply write:

~~~~~~~~
if (condition) {
    statement
}
~~~~~~~~

Finally, there is also an `else if`, in case you have multiple conditions:

~~~~~~~~
if (condition1) {
    statement1
}
else if (condition2) {
    statement2
}
else {
    statement3
}
~~~~~~~~

If `condition1` is true, then only `statement1` is executed. If `condition1` is false, but `condition2` is true, then `statement2` is executed. If both conditions are false, then `statement3` is executed.

Sometimes, you see this type of code:

~~~~~~~~
if (condition) 
    statement1;
~~~~~~~~

This is technically correct - if the statement is only one line of code, you can theoretically omit the curly brackets. *Don’t ever do this* - it will almost certainly introduce nasty errors down the line. You may for example want to add a second statement later on, and may end up doing something like this:

~~~~~~~~
if (condition) 
    statement1;
    statement2;
~~~~~~~~

You might think that `statement2` will be executed only if `condition` is true, but that would be wrong. The code above is equivalent to:

~~~~~~~~
if (condition) { 
    statement1;
}
statement2;
~~~~~~~~

This means that `statement2` will always be executed, which is not at all what you wanted. There’s a simple rule to avoid this danger: *always use curly braces when you use if and else statements*.

We need to understand one more thing about control flow - that of the condition itself. A condition must always evaluate to be either true or false. Like all programming languages, JavaScript provides a special type called *boolean*, which can either be true or false (the other two types you have encountered so far were numbers and strings). You can use this type in normal variables, like so:

~~~~~~~~
var is_ready = false;
~~~~~~~~

A condition, on the other hand, is not a variable that you set yourself - it’s an expression that is either true or false. I’m going to show you a few examples of expressions that evaluate to true or false, and while there are many more, these are the ones you need when dealing with numbers.

Smaller than:  
`3 < 4` evaluates to true  
`4 < 3` evaluates to false

Smaller or equal than:  
`4 <= 4` evaluates to true  
`5 <= 4` evaluates to false

Greater than:  
`4 > 3` evaluates to true  
`3 > 4` evaluates to false

Greater or equal than:  
`4 >= 4` evaluates to true  
`4 >= 5` evaluates to false

Equals:  
`4 == 4` evaluates to true  
`4 == 5` evaluates to false

This last expression is the source of most beginner’s mistakes. To compare two values and test their equality, you use the `==` operator. This operator is deceptively similar to the assignment operator `=` that we have used many times before, when assigning a value to a variable.

Now back to our coin tossing example - the code should be easy to understand now. It basically says that if `Math.random()` returns a number that is smaller than or equal to `0.5`, the counter for `heads` will increase by `1`. Otherwise, the counter for `tails` will increase by `1`.

At the moment, our code simply tosses a coin ten times and then prints how often it came up heads or tails. What we want to do, however, is to figure out whether my original claim - that the probability of 80% heads 20% tails is 4.39% - is correct. What we need to do, then, is to repeat our code thousands of times, and count how often we get exactly 8 heads and 2 tails. So the first thing I’m going to do is to wrap the coin tossing functionality into a function that I’m going to name `throw_coins()`:

~~~~~~~~
function throw_coins() {
    var throws = 10;
    var heads = 0;
    var tails = 0;
    for (var i = 0; i < throws; i = i + 1) {
        if (Math.random() <= 0.5) {
            heads = heads + 1;
        }
        else {
            tails = tails + 1;
        }
    }
    if (heads == 8) {
        return true;
    }
    else {
        return false;
    }
}
~~~~~~~~

You’ll recognize the first part of the function - it’s an exact copy of the code that we developed above. But then I’ve removed the line with the `console.log` method, since there is no need to print the outcome every time we throw the coin 10 times. However, what we need instead is for the function to somehow tell us what happened. So what I added there at the end simply says if heads comes up 8 times, then return `true`, otherwise, return `false`.

Now I can call this function however many times I want to, and then count how many times the function returns `true`, which indicates that the 10 coin tosses resulted in exactly 8 heads and 2 tails. Here’s how I do this:

~~~~~~~~
var repeats = 10000000;
var counter = 0;
for (var i = 0; i < repeats; i = i + 1) {
    var desired_outcome = throw_coins();
    if (desired_outcome) {
        counter = counter + 1;
    }
}
console.log("Getting 8 heads, 2 tails " + (counter / repeats) * 100 + "% of the time”)
~~~~~~~~

Be sure to set the `repeats` variable to a value that allows your computer to execute the code in just a few seconds. It’s best to start with a small value like 1000 and then increase it by an order of magnitude, as we have done before. In my case, the sweet spot seems to be 10 million.

The setup here should look familiar - you initialize the number of repeats in a variable called `repeats`, then initialize a variable `counter` at `0`, and then iterate using a `for` loop. In the loop, I call the function `throw_coins()`, and whatever it returns gets stored in the variable `desired_outcome`. If that happens to be `true` - which means that the 10 coin tosses resulted in 8 heads and 2 tails - I am going to increase the `counter` by 1. Once the loop has run its course, I’m simply printing how often, in percentage, 10 coin tosses resulted in 8 heads and 2 tails.

Thus, our full code is as follows:

~~~~~~~~
function throw_coins() {
    var throws = 10;
    var heads = 0;
    var tails = 0;
    for (var i = 0; i < throws; i = i + 1) {
        if (Math.random() <= 0.5) {
            heads = heads + 1;
        }
        else {
            tails = tails + 1;
        }
    }
    if (heads == 8) {
        return true;
    }
    else {
        return false;
    }
}

var repeats = 10000000;
var counter = 0;
for (var i = 0; i < repeats; i = i + 1) {
    var desired_outcome = throw_coins();
    if (desired_outcome) {
        counter = counter + 1;
    }
}
console.log("Getting 8 heads, 2 tails " + (counter / repeats) * 100 + "% of the time")
~~~~~~~~

When I run this code three times (i.e. reload the page three times) I’m getting the following output:

~~~~~~~~
Getting 8 heads, 2 tails 4.39729% of the time 
Getting 8 heads, 2 tails 4.39629% of the time 
Getting 8 heads, 2 tails 4.38968% of the time
~~~~~~~~ 

Thus, 4.39% is indeed correct.

##The randomness of finite populations

So how is life random, in the biological sense? Think about the circle of life in most animals - new life is conceived, a single fertilized egg (the zygote) multiplies and the animal grows to the reproductive age, reproduces, and eventually dies. Its offspring goes through the same circle, and so does its offspring, and so on. Randomness can hit the animal at any stage from the fertilized egg (they zygote) to the moment it reproduces, and a new zygote is created. Which genes get passed on will be largely random. When a single cell grows into billions of cells forming an adult capable of reproduction, a lot can go wrong due to randomness. At any time, an animal can  struck by a deadly disease, be eaten by a predator, die of hunger and thirst, etc. 

Life is complicated, and if we would set out to simulate life in all its details, we would end up with a very complicated model. But just as we have done in the previous chapter, we can again reduce the complexity into a simple model that captures the essence of these random processes. Once again, the model is named after the two originators, Sewall Wright and Ronald Fisher: it's called the *Wright-Fisher model*.

Before we go on, it’s important to remind ourselves that the only assumption we are relaxing from the Hardy-Weinberg model is that of the infinite population size. We still have random mating, we still have no selection, etc. *We are only interested in the effect of a finite population size.*

The complexity of the scenario described above can be reduced to a simple sampling process. Imagine that we have a finite population of N individuals. Once again, let’s zone in on one gene, with two alleles. If we have N diploid individuals, that means we have 2N copies of the alleles in the population (because every individual has two copies). Once again, we assume that individuals produce infinitely many gametes from which the next generation is formed. However, this time, we don’t form infinitely many offspring individuals - we stick to our population size of N individuals. 

This process can be thought of as having a jar with 2N marbles in it (the marbles being the actual allele copies of the N individuals). To generate the next generation, we *sample* - we simply pick a marble at random (representing a random gamete, i.e. one particular allele), and then put it back into the jar. This “replacement” step ensures that we are sampling from an infinitely large pool of gametes. To generate an individual, we need to repeat this process, because each individual consists of two gene copies. Once we have picked two marbles in that fashion, we will have generated one offspring genotype. Great! But in order to produce the next generation, which should have a population size of N, we need to keep repeating this procedure, until we have picked (or sampled) 2N allele copies, representing the next generation.

*This random sampling process can have profound consequences.* Let’s go through an example, with N = 10. This is arguably an extremely small population, but it helps us keep the example manageable. Let’s also assume that we start with p = q = 0.5, which means that the allele frequencies of both alleles, A1 and A2, are equally 50%. Starting with this population, what will the next generation look like? Let us remind here what we have established in the previous chapter: in an infinite population under the same conditions (i.e. Hardy Weinberg conditions), the allele frequencies would not change - they would stay at 50%  for ever. In other words, no evolution would occur. 

Starting from 10 individuals, we have 20 allelic copies in the population, half of them are A1, and half of them are A2. We don’t care about the genotype frequencies at the moment - this chapter is all about allele frequencies. In order to generate the next generation of 10 individuals, or 20 allelic copies, we need to randomly sample, with replacement, from the marble jars representing the infinite gamete pool.

Let’s get started. We grab the first allele - it’s an A1! (Recall that the chance is 50/50, given the allele frequencies in the parent generation.) Ok, let’s copy this A1 allele, put it back in the jar, and grab another one. Again and A1! Let’s copy it again, put it back in the jar, and grab another one. An A2! And so on.

After we do this 20 times, we assess our new allele pool. Let's say we have drawn allele A1 12 times, and allele A2 8 times. In other words, p, the frequency of the A1 allele, is now 0.6, and q, the frequency of the A2 allele, is now 0.4. 

Let me repeat this. p is now 0.6, and q is now 0.4.

Ok, let me repeat this again. p is now 0.6, and q is now 0.4.

Mind blown? It should be.

Let’s consider what just happened. In a single generation, we went from p = q = 0.5 to p = 0.6 and q = 0.4. That is, in absolute terms, a dramatic change of allele frequencies, in a single generation. And since you now know that a change of allele frequencies is pretty much the definition of evolution, another way of saying this is that we have just observed a dramatic evolutionary change in one single generation. “Fair enough”, I hear you say, “but we are talking about a very small population of 10 animals. Certainly these effects are much weaker in larger populations.” That is true - but the effect is still there. And keep in mind, we have observed only a single generation. What happens over evolutionary timescales? Thousands, hundreds of thousands, millions of generations? Even small changes in allele frequencies could add up.

And here is where we go back to code. We can, in fact, simulate the dynamics of larger populations over many generations. We will be using the exact same approach that we’ve taken above in the coin tossing example. Our goal is to simulate the allele frequencies over time, starting from p = q = 0.5, in a population of 1000 individuals. Here is the code: 

~~~~~~~~
var p = 0.5;
var N = 1000;
var generations = 1000;

function next_generation() {
    var draws = 2 * N;
    var A1 = 0;
    var A2 = 0;
    for (var i = 0; i < draws; i = i + 1) {
        if (Math.random() <= p) {
            A1 = A1 + 1;
        }
        else {
            A2 = A2 + 1;
        }
    }
    p = A1/draws;
}

for (var i = 0; i < generations; i = i + 1) {
    next_generation();
    console.log("generation "+i+":\tp = " + round_number(p,3) + "\tq = " + round_number(1-p,3));
}
~~~~~~~~

(Note that I’m omitting the `round_number()` method for brevity - you can just copy it from the previous chapter).

The function `next_generation()` is almost identical to the function `throw_coins()1 from above, with a small but important difference. Instead of saying 

~~~~~~~~
if (Math.random() <= 0.5)
~~~~~~~~

we are using

~~~~~~~~
if (Math.random() <= p)
~~~~~~~~

The first line is correct if you want to do something 50% of the time. However, when the frequency of allele A1 is p, then the second line is correct. Imagine for example that the frequency of allele A1 is `0.8`. This means that when you randomly sample from the gametic pool, you will pick an A1 allele 80% of the time. Because p will be `0.8`, the line 

~~~~~~~~
if (Math.random() <= p)
~~~~~~~~

is equivalent to 

~~~~~~~~
if (Math.random() <= 0.8)
~~~~~~~~

which is saying “80% of the time” (because indeed, 80% of the time, `Math.random()` will return a number smaller than or equal to `0.8`).

This is an important line, make sure you understand it. It’s your key to programming stochastic events. 

With the code saved in a new HTML document, reload that document and look at the output in the JavaScript console. It will output one thousand lines, representing the allele frequencies over 1000 generations (i.e. evolution). When I run this a couple of times, I’m getting the following frequencies at generation 999:

~~~~~~~~
generation 999:	p = 0.886	q = 0.114 

generation 999:	p = 0.585	q = 0.415 

generation 999:	p = 0	q = 1 

generation 999:	p = 0.953	q = 0.047 

generation 999:	p = 0.124	q = 0.876 
~~~~~~~~

and so on. If you scroll through the generations, you can see that the allele frequencies are changing wildly. In other words, there is a lot of evolution going, despite the complete lack of natural selection. This is genetic drift - evolution due to chance.

The third simulation from my examples above has a pretty remarkable outcome: the allele A1 has completely disappeared from the population, and all alleles are A2. In fact, when I scroll back through time in this simulation, I find the following:

~~~~~~~~
...
generation 771:	p = 0.003	q = 0.997
generation 772:	p = 0.002	q = 0.998
generation 773:	p = 0.001	q = 0.999
generation 774:	p = 0	q = 1
generation 775:	p = 0	q = 1
generation 776:	p = 0	q = 1
generation 777:	p = 0	q = 1
generation 778:	p = 0	q = 1
generation 779:	p = 0	q = 1
generation 780:	p = 0	q = 1
generation 781:	p = 0	q = 1
...
~~~~~~~~

At generation 774, the allele A1 is completely lost from the population, and after that, it will never come back into the population again (because our assumptions say we have no mutation and no migration). This is what loss of genetic diversity looks like in mathematical terms. 

Now go ahead and change your code to say that the simulation should run for ten thousand generations, not just for a thousand generations. That is, change this line 

~~~~~~~~
var generations = 1000;
~~~~~~~~

to

~~~~~~~~
var generations = 10000;
~~~~~~~~

and run the simulation again. Because we increased the number of generations by an order of magnitude, the simulation will now take longer, but did you notice the pattern in the results? I’m sure you did, because it’s really hard to miss: in (almost) all cases, one of the two alleles will disappear entirely from the population. There might be the rare simulation where you still have both alleles after 10,000 generations, but in my 50 runs or so, one of the alleles had always disappeared by the end of the simulation.

This is a remarkable result, and it is perhaps one of the key insights about genetic drift, which is this: *genetic drift reduces genetic variation*. This may sound a little counterintuitive at first, because we’re accustomed to think that randomness, or stochasticity, leads to more variation, not less. But you can observe the pattern very clearly in the results of your simulations. Why is that?

Fundamentally, genetic drift cannot add more variation - it is simply a random sampling process, unable to generate variation on its own. At the same time, because it is a random sampling process, alleles can eventually be removed, thus reducing variation. 

As we established in the beginning of this chapter, stochastic effects are strongest when population sizes are smaller. Extreme results, like throwing only heads, are much more likely when you are throwing the coin only a few times. In the same vein, when the population size is small, loss of an allele is much more likely, and will thus occur much sooner, than when the population size is big. Recall from above that at population size `N=1000`, only rarely was an allele lost within `1000` generations, typically. Go ahead and change your population size to `100` (also be sure to set the simulation to run for `1000` generations only), and you will see that in practically all cases, one of the alleles will be lost.

These simulations are a great tool to examine the dynamics of genetic drift, but it’s bit cumbersome to scroll though hundreds or thousands of lines of allele frequencies. Wouldn’t it be nice if we had a way to visualize the allele dynamics? Let’s start visualizing things to get a better overview on what’s going on here.

Before we get to it, a word of warning: Data plotting in the browser isn’t trivial. What I’m going to do here is to give you some plotting code that I’m simply asking you to copy and paste into your documents. The code that we will continue to write simply generates the data, and we then hand the data over to a plotting function. There’s no need for you at this stage to understand how plotting works, and it would be a significant distraction at the moment. Feel free to examine the plotting code, of course, but I won’t explain it, nor do I expect you to understand it.

First, add this line to your code, at the beginning of your `<head>` element:

~~~~~~~~
<script src=“http://d3js.org/d3.v3.js"></script>
~~~~~~~~

This line loads and external JavaScript library called `D3.js`, which is the most advanced data plotting library available for the browser to date. Note that you need to be connected to the internet for this library to be loaded!

Next, in your already existing `<script>` tag, put the following plotting code at the top: 

~~~~~~~~
 function draw_line_chart(data,N,generations) {
      var margin = {top: 20, right: 20, bottom: 50, left: 50},
              width = 700 - margin.left - margin.right,
              height = 400 - margin.top - margin.bottom;


      var x = d3.scale.linear()
              .range([0, width]);

      var y = d3.scale.linear()
              .range([height, 0]);

      var xAxis = d3.svg.axis()
              .scale(x)
              .orient("bottom");

      var yAxis = d3.svg.axis()
              .scale(y)
              .orient("left");

      var line = d3.svg.line()
              .x(function (d, i) {
                return x(i);
              })
              .y(function (d) {
                return y(d);
              });

      var svg = d3.select("body").append("svg")
              .attr("width", width + margin.left + margin.right)
              .attr("height", height + margin.top + margin.bottom)
              .append("g")
              .attr("transform", "translate(" + margin.left + "," + margin.top + ")");


      x.domain(d3.extent(data, function (d, i) {
        return i;
      }));

      svg.append("g")
              .attr("class", "x axis")
              .attr("transform", "translate(0," + height + ")")
              .call(xAxis)
              .append("text")
              .attr("text-anchor", "middle")
              .attr("x", width / 2)
              .attr("y", 6)
              .attr("dy", "3em")
              .text("Generation");

      svg.append("g")
              .attr("class", "y axis")
              .call(yAxis)
              .append("text")
              .attr("transform", "rotate(-90)")
              .attr("x", -height / 2)
              .attr("dy", "-3.5em")
              .style("text-anchor", "middle").
              text("p");

      svg.append("text")
              .attr("class", "y label")
              .attr("text-anchor", "end")
              .attr("x", width / 2)
              .attr("y", 6)
              .attr("dy", ".75em")
              .attr("transform", "rotate(-90)")
              .text("p");

      var legend = svg.append("text")
              .attr("text-anchor", "star")
              .attr("y", 30)
              .attr("x", width-100)
              .append("tspan").attr("class", "legend_title").text("Population Size:")
              .append("tspan").attr("class", "legend_text").attr("x", width-100).attr("dy", 			20).text(N)
              .append("tspan").attr("class", "legend_title").attr("x", width-100).attr("dy", 			20).text("Generations:")
              .append("tspan").attr("class", "legend_text").attr("x", width-100).attr("dy", 			20).text(generations);


      svg.append("path")
              .datum(data)
              .attr("class", "line")
              .attr("d", line);
    }
~~~~~~~~


Like I said, let’s not bother to understand this code. All we care about is that it defines a function called `draw_line_chart()` with three parameters: `data`, `N` and `generations`. The `data` corresponds to the data that we generate in the simulation, i.e. the frequencies of the A1 allele over time. `N` and `generations` simply correspond to the population size, and the number of generations that we want the simulations to run. The function then takes those arguments, and plots the dynamics of our simulation in the browser.

In order for the plots to look pretty, copy this code into your somewhere within your `<head>` element:  

~~~~~~~~
<style>

  body {
    font: 10px sans-serif;
  }

  .axis path,
  .axis line {
    fill: none;
    stroke: #000;
    shape-rendering: crispEdges;
  }

  .x.axis path {
    display: none;
  }

  .line {
    fill: none;
    stroke: steelblue;
    stroke-width: 1.5px;
  }

  .legend_title {
    font-size:12px;
    fill:#555;
    font-weight:400;
  }

  .legend_text {
    font-size:20px;
    fill:#bbb;
    font-weight:700;
  }

</style>
~~~~~~~~

This is not JavaScript code; it is *CSS*, which stands for cascading style sheets, and it is a very common “language” to stylize data, i.e. to define colors, font sizes, line thickness, etc. Once again, because it is only used to help us visualize the data, I won’t go into the details here. 

With these things in place, we are now ready to plot our simulation results. 

Let’s take our simulation code form above, and modify it slightly:

~~~~~~~~
var p = 0.5;
var N = 500;
var generations = 1000;
var data = [];

function next_generation() {
    var draws = 2 * N;
    var A1 = 0;
    var A2 = 0;
    for (var i = 0; i < draws; i = i + 1) {
        if (Math.random() <= p) {
            A1 = A1 + 1;
        }
        else {
            A2 = A2 + 1;
        }
    }
    p = A1/draws;
    data.push(p)
}

for (var i = 0; i < generations; i = i + 1) {
    next_generation();
}
draw_line_chart(data,N,generations);
~~~~~~~~

I’ve made a few changes, which I’m going to explain next. First, after I’ve played around with various values for `N` and `generations` above, I’ve set them to `500` and `1000`, respectively.

Next, I initialized an empty *array* called `data`. with the following line:

~~~~~~~~
var data = [];
~~~~~~~~

Arrays are the most important data structures in JavaScript, and I’ll talk more about arrays below. For now, simply note that you can store multiple values (e.g. multiple numbers) in an array. We’ll use it to store the values of p over time.

In the function `next_generation()`, I’ve added this line at the end: 

~~~~~~~~
data.push(p)
~~~~~~~~

which simply adds `p` to the `data` array. Next, I’ve removed the line the prints the values of `p` and `q` to the JavaScript console, but you can leave it in there if you want to. Finally, I’ve added this line:

~~~~~~~~
draw_line_chart(data,N,generations);
~~~~~~~~

This is the line that calls the function `draw_line_chart()` with the parameters `data`, `N` and `generations`. Note that I’m calling the function at the end of the code - at that point, the array `data` is not empty anymore, but in fact contains all `p` values over the `1000` generations. It’s that set of values that we hand over to the function, and the function then plots the data visually.

Save the document and reload the browser. You should see a nice line chart showing the change of `p` over time:
 
In this particular simulation run, the allele A1 went to fixation around generation 850, which is another way of saying that its frequency reached 100%. This also means that the allele A2 was lost at the same time (whenever an allele goes to fixation, all other alleles are lost). Reload the page and watch different evolutionary dynamics unravel. As you can observe, the dynamics are completely random, as we expected. Sometimes, one of the allele goes relatively straight to fixation. Other times, the frequencies fluctuate widely, with one allele almost driven to extinction, only to make a dramatic comeback, and then still being lost 200 generations later.

With this code in place, let’s focus on the effect of population size. Change the population size to 10, and reload the page. You should see something like this:
 


Again, yours will look different each time you reload the page, but you’ll note that one of the alleles will go to fixation very rapidly, sometimes within just a few generations. Now increase the population size to 50, and you’ll get a result like this: 

With `N = 50`, the time to loss / fixation is a little longer, but it’s still shorter than with `N = 500`. If you play around with different numbers, you’ll notice the general pattern that we already observed earlier: that the random effects are much stronger when the population is smaller, and as a consequence, allele loss / fixation happens faster.

Rather than reloading the page and seeing a single simulation run its course, I want to modify the code so that we can run a number of simulations and plot them all at once. In order to do this though, I first need to explain in more detail the concept of an array. I mentioned above that we are using a JavaScript array that we name data, and the we initialize to be an empty array, like so:

~~~~~~~~
var data = [];
~~~~~~~~

So what is an array? An array is simply a list-like object that contains any number of so-called *elements*. For example, this is an array with five elements:

~~~~~~~~
var arr = [4,2,6,8,3];
~~~~~~~~

The `data` array that we used above contained zero elements - it is an empty array. Arrays use brackets, i.e. `[` and `]`, to wrap the elements which are themselves separated by commas.

Why are arrays important? The variable types that we encountered so far - numbers, strings, and booleans - can only ever hold one single value, like `123`, `"hello there"`, or `true`. But whenever you are dealing with data, you have many values, and you need a way to store them somehow in code. That’s where arrays come in. The array `arr` in the example above holds 5 values, which happen to be numbers. An array can store any other type, and even mix them, so you could have an array like this:

~~~~~~~~
var b = [4,"hello there",6,true,false];
~~~~~~~~

Now that you know what an array is, how to do you manage it? How do you add data to it, and how to retrieve it later?

There are two ways by which you can add data to an array. The first way is to do it when you initialize the array, as we’ve done with array `arr` above. The second way is to *push* data into an existing array, as we’ve done in the genetic drift code example above, like so:

~~~~~~~~
arr.push(5);
~~~~~~~~

This line adds the number five to the list of numbers in the array. Importantly, it will be added *at the end* of the list, so that the new array will contain the values 4, 3, 6, 8, 3 and 5, in that order.

You can retrieve elements by using the concept of an *index*. If you want the n^th^ element of an array `arr`, you retrieve like so:

~~~~~~~~
arr[n]
~~~~~~~~

However, there is one major gotcha: like in almost any data structure in almost any programming languages, JavaScript arrays are zero-indexed. This means that the first element has index 0, not index 1. If you are new to programming, this is going to be a major source of bugs, and probably hours of desperation. Sorry to be so blunt, but it is quite true. Perhaps this dramatic warning will at least remind you what to look for when your code behaves strangely.

Thus, in order to retrieve the first element in the array `arr`, you would use `arr[0]`; in order to retrieve the second element, you would use `arr[1]`; and so on.

Here where it gets interesting: I mentioned above that you can store any type of variable in an array. What that means is that you can also store *other arrays*, allowing you to create *multi-dimensional arrays*. Imagine you want to store three sets of five numbers in an array. You could do it as follows:

~~~~~~~~
var set1 = [1,2,3,4,5];
var set2 = [10,11,12,13,14];
var set3 = [33,44,55,66,77];

var data = [set1, set2, set3];
~~~~~~~~

Alternatively, you could initialize the data array directly, like so:

~~~~~~~~
var data = [[1,2,3,4,5], [10,11,12,13,14], [33,44,55,66,77]];
~~~~~~~~

If you need access the subsets, you would then do it as before: `data[0]` accesses the first element in the array, which is `[1,2,3,4,5]`. 

If you want to access a number in one of the arrays, you would first have to access the corresponding array, and then access the element that you want from that array. For example, in order to get the number 33 from the data array, you would have to write `data[2][0]`.

Multidimensional arrays sound complicated, but they are very simple, as you've just learned. They are also very handy when you deal with more complex datasets. For example, when we want to store the dynamics of multiple simulation runs, rather than just one, we can simply use a multidimensional array. The main array would then contain all the arrays for each simulation run, and those in turn would contain the allele frequencies for each generation.

There’s one more thing I want to mention about arrays. If you want to know how many elements and array currently holds, you can just use the following syntax:

~~~~~~~~
data.length
~~~~~~~~

This often comes in handy when you need to set up a loop to iterate over an array, and you need to know how many elements you have in the array.

Going back to our code, let’s go ahead and change our simulation slightly to allow for multiple simulation runs.

Here is the new code:

~~~~~~~~
var p;
var N = 20;
var generations = 200;
var data = [];
var simulations = 10;


function next_generation(simulation_data) {
    var draws = 2 * N;
    var A1 = 0;
    var A2 = 0;
    for (var i = 0; i < draws; i = i + 1) {
        if (Math.random() <= p) {
            A1 = A1 + 1;
        }
        else {
            A2 = A2 + 1;
        }
    }
    p = A1/draws;
    simulation_data.push(p);
}

function simulation(simulation_counter) {
    p = 0.5;
    for (var i = 0; i < generations; i = i + 1) {
        next_generation(data[simulation_counter]);
    }
}

for (var i = 0; i < simulations; i = i + 1) {
    data.push([]);
    simulation(i);
}

draw_line_chart(data,N,generations);
~~~~~~~~

Let’s go through the changes here. First, I define the global variable `p`, but don’t assign it a value any more. That’s because every simulation should start again with a new value, so it’ll be part of the simulation code to set `p` to `0.5` every time a new simulation begins. I’m also introducing a new variable `simulations` that defines how many simulations we’re going to run.

The function `next_generation()` has one subtle but important change. It now has an argument called `simulation_data`, and in the last line, the allele frequency `p` is added to `simulation_data`. (i.e. `simulation_data` is expected to be an array). I’ll get back to that in a second.

The loop where we call the `next_generation()` function is now encapsulated in a function called `simulation()`. This function is also where we make sure to (re)set the value of `p` to `0.5` every time we start a new simulation. It also has one argument, `simulation_counter`, which we use as an index for the data array. 

All of this should become clear on the next few lines, where we have a loop that iterates as many times as we have simulations. In each iteration, we add an empty array to the `data` array. This empty array is the array that will contain all the A1 allele frequency values for one simulation run. Then, we call the function simulation, with the value `i` as an argument, which is our current counter for the simulations (starting at `0`). So when the function `simulation()` is called, `simulation_counter` will be set to whatever `i` was, and `data[simulation_counter]` thus corresponds to the array for a given simulation. It’s that array that we pass as an argument to the function `next_generation()`.

At the end, we call the `draw_line_chart()` function as before - but importantly, this time we will be passing it a multidimensional array (unlike in the example before, where we passed it a one-dimensional array containing only the results of a single simulation run).

Our drawing code - which I previously asked you to cut and paste - needs to be updated as well to handle multiple simulations. So once again, I’m going to ask you to cut and past the following, updated drawing code, replacing the function `draw_line_chart`:

~~~~~~~~
function draw_line_chart(data,N,generations) {
    var margin = {top: 20, right: 20, bottom: 50, left: 50},
    width = 700 - margin.left - margin.right,
    height = 400 - margin.top - margin.bottom;


    var x = d3.scale.linear()
              .range([0, width]);

    var y = d3.scale.linear()
              .range([height, 0]);

    var xAxis = d3.svg.axis()
              .scale(x)
              .orient("bottom");

    var yAxis = d3.svg.axis()
              .scale(y)
              .orient("left");

    var line = d3.svg.line()
              .x(function (d, i) {
                return x(i);
              })
              .y(function (d) {
                return y(d);
              });

     var svg = d3.select("body").append("svg")
              .attr("width", width + margin.left + margin.right)
              .attr("height", height + margin.top + margin.bottom)
              .append("g")
              .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

    var color = d3.scale.category10();

    x.domain(d3.extent(data[0], function (d, i) {
        return i;
    }));


    svg.append("g")
              .attr("class", "x axis")
              .attr("transform", "translate(0," + height + ")")
              .call(xAxis)
              .append("text")
              .attr("text-anchor", "middle")
              .attr("x", width / 2)
              .attr("y", 6)
              .attr("dy", "3em")
              .text("Generation");


    svg.append("g")
              .attr("class", "y axis")
              .call(yAxis)
              .append("text")
              .attr("transform", "rotate(-90)")
              .attr("x", -height / 2)
              .attr("dy", "-3.5em")
              .style("text-anchor", "middle").
              text("p");

    svg.append("text")
              .attr("class", "y label")
              .attr("text-anchor", "end")
              .attr("x", width / 2)
              .attr("y", 6)
              .attr("dy", ".75em")
              .attr("transform", "rotate(-90)")
              .text("p");

    var legend = svg.append("text")
              .attr("text-anchor", "star")
              .attr("y", 30)
              .attr("x", width-100)
              .append("tspan").attr("class", "legend_title").text("Population Size:")
              .append("tspan").attr("class", "legend_text").attr("x", width-100).attr("dy", 			20).text(N)
              .append("tspan").attr("class", "legend_title").attr("x", width-100).attr("dy", 			20).text("Generations:")
              .append("tspan").attr("class", "legend_text").attr("x", width-100).attr("dy", 			20).text(generations);


    var simulation = svg.selectAll(".simulation")
              .data(data)
              .enter().append("g")
              .attr("class", "simulation");

    simulation.append("path")
              .attr("class", "line")
              .attr("d", function(d) { return line(d); })
              .style("stroke", function(d,i) { return color(i); });

    }
~~~~~~~~

With this new function in place, save your HTML file and reload the page. You will see 10 simulation runs at the same time. 

With that in place, let us run this code three times for 200 generations, with three different population sizes: `N = 20`, `N = 200`, and `N = 2000`.

Here is what these three sets of simulation will look like:

N=20:




N=200:





N=2000:




These figures are telling. They show that the smaller the population, to more pronounced the random evolutionary swings in each generation. Because of that, alleles will go to fixation in smaller populations much faster.

Now that we have a good intuition from the simulations, let’s see if we can develop a mathematical model that captures this process. Don’t worry, the math is going to be easy, and I will be explaining every step in great detail.

##A mathematical formulation of genetic drift

We have just established, through stochastic simulations, that the main effect of genetic drift is the reduction of genetic variance. We are now looking to find a way to capture this process mathematically.

Let’s go back to our marble jar example. Recall that we had all the allele copies of the population in a jar, and in order to generate the next generation, we would pick a random marble, copy it, and put it back into the jar. We would do this *2N* times in total. 

Let’s define *G* as the probability that two alleles picked randomly from the population of *2N* alleles are identical by state (in our example, that would mean they are both A1s, or both A2s). Thus *G* is the probability that if you pick any two marbles from the jar, at the same time, they would both be A1s, or both be A2s.

We are now in a position to ask ourselves, if the current generation has a certain value of *G*, what is *G* going to be in the next generation? We’re going to denote *G* in the next generation simply as *G’*, to avoid confusion with *G* in this generation.

There are two ways by which two randomly picked marbles are identical by state. It could be that they had the same ancestor marble that was simply picked twice. The probability of this is {$$}\frac{1}{2N}{/$$}. Why? Because once you pick a certain marble from a jar out of *2N* marbles, and put it back again, the possibility that you pick that exact same marble again next time is {$$}\frac{1}{2N}{/$$} (since there are *2N* marbles you can pick from).

The second way by which two randomly picked marbles are identical by state is that they were did not have the same ancestor marble, but their ancestors just happened to be the same type (i.e. either both ancestors were A1 or both were A2). The probability of that is {$$}1-\frac{1}{2N}G{/$$}, and here’s why. First, the probability that two marbles did not have the same ancestor is {$$}1-\frac{1}{2N}{/$$}. We know this because we have just established above that the probability that they do have the same ancestor is {$$}\frac{1}{2N}G{/$$}, so the probability that they *don’t* have the same ancestor is 1 minus that, i.e. {$$}1-\frac{1}{2N}{/$$}. But that’s not the end of the story - the two alleles also need to be identical in state, which is *G*. Thus the total probability is the product of these two probabilities, i.e. {$$}1-\frac{1}{2N}G{/$$}.

Having established the two (mutually exclusive) ways by which two randomly picked marbles can be identical, considering what *G* is in this generation, we simply need to add them up to get the total probability, which is 

{$$} 
\frac{1}{2N} + (1-\frac{1}{2N})G
{/$$} 

which corresponds to *G’* in the next generation.

What we have just done is started from the probability that two randomly picked alleles in this generation are the same with probability *G*, and then established that in the next generation, *G’* will be {$$}\frac{1}{2N} + (1-\frac{1}{2N})G{/$$}. This is the most important part of the story - everything that follows now is just algebraic reformulation.

Let’s start by defining *H* as *H = 1-G*. Thus, *H* is the probability that two alleles picked randomly from the population of *2N* alleles are different (because *G* was the probability that they are the same). This is a good measure for genetic variance. If you have high genetic variation, then the chance that two randomly picked alleles are different is high. Conversely, if you have low genetic variation, then the chance that two randomly picked alleles are different is low. In the worst case, you have no genetic variation (*H = 0*) - all alleles are the same type, and the chance that two randomly picked alleles are different is zero.

Because *H = 1 - G*, the same must also be true in the next generation, i.e.

{$$} 
H’ = 1 - G’
{/$$} 

since we know that 

{$$} 
G’ = \frac{1}{2N} + (1-\frac{1}{2N})G
{/$$} 

we can replace this term in the equation above and write 

{$$} 
H’ = 1 - (\frac{1}{2N} + (1-\frac{1}{2N})G)
{/$$} 

Because *H = 1-G*, it’s also true that *G = 1-H*, and we can thus replace the *G* in the previous equation as well, and write

{$$} 
H’ = 1 - (\frac{1}{2N} + (1-\frac{1}{2N})(1-H))
{/$$} 

If we expand the terms in the equation, we get

{$$} 
H’ = 1 - (\frac{1}{2N} + 1 - H - \frac{1}{2N} + \frac{H}{2N})
{/$$} 

removing the parentheses yields

{$$} 
H’ = 1 - \frac{1}{2N} - 1 + H + \frac{1}{2N} - \frac{H}{2N}
{/$$} 

which makes various terms fall out of the equation, leaving only

{$$} 
H’ = H - \frac{H}{2N}
{/$$} 

Let’s factor out *H*:

{$$} 
H’ = (1 - \frac{1}{2N})H
{/$$} 

which is an equation that looks unassuming - but is indeed one of the most profound equations of evolutionary biology, immediately revealing two major aspects about genetic drift. First, it tells us that no matter what *H* is in this generation, in the next generation it is going to be smaller. That’s because {$$}(1 - \frac{1}{2N}){/$$} will always be smaller than 1, and in order to get *H’*, we need to multiply *H* by something that is smaller than 1, which means that *H’ < H*. Boom -  right there, we’ve shown mathematically that the effect of genetic drift is to reduce genetic variation. But this formula is not done yet! It reveals another truth, which is that the smaller the population size *N*, the stronger the reduction in genetic variation per generation. Why? Well, we just said that in order to get *H’*, we need to multiply *H* by something that is smaller than 1 - but how much smaller? As you can easily see, the term {$$}(1 - \frac{1}{2N}){/$$} will be smaller if *N* is small. And it will be closer to 1 if *N* is large. 

Two major insights based on a simple equation, that we were able to derive from simple principles, with simple algebra. Not too shabby! But wait, there’s more!

Let’s denote the genetic variance and generation 0 with {$$}H_0{/$$}, and thus

{$$}
H_1 = (1 - \frac{1}{2N}) H_0
{/$$}

For the next generation, we have

{$$}
H_2 = (1 - \frac{1}{2N}) H_1
{/$$}

Since we know the formula for {$$}H_1{/$$}, we can just replace it and write

{$$}
H_2 = (1 - \frac{1}{2N}) (1 - \frac{1}{2N}) H_0
{/$$}

Similarly, in generation 3, we’ll have

{$$}
H_3 = (1 - \frac{1}{2N}) H_2
{/$$}

Again, since we know the formula for {$$}H_2{/$$}, we can just replace it and write

{$$}
H_3 = (1 - \frac{1}{2N}) (1 - \frac{1}{2N}) (1 - \frac{1}{2N}) H_0
{/$$}

and so on. For every generation, you simply have to multiply by {$$}(1 - \frac{1}{2N}){/$$} again, leading to the more generic form

{$$}
H_t = (1 - \frac{1}{2N})^t H_0
{/$$}

What this means is that if *t* is large enough - in other words, over sufficiently long time - the term {$$}(1 - \frac{1}{2N})^t{/$$} will approach 0, which means genetic variation will approach 0, and all alleles but one will be lost. This is another significant insight from just this one formula. Unless other forces counter it, genetic drift will reduce all genetic variation in every population.

And as if those three insights were not enough, let’s develop a fourth insight. We know that drift reduces genetic variation; we know that the effect of drift is strongest in smaller populations; and we know that the ultimate outcome of drift would be the fixation of one allele, and hence a total destruction of genetic variance, unless other process counter it. One question remains open, though - how long does all of this take?

As with any process of decay, the best way to address this problem is by asking how long it takes to reduce the current genetic variance by half. You may have come across this concept as that of half-life - i.e. the amount of time required for a quantity to fall to half of its initial value - in physics or chemistry.

We can turn this question into a mathematical equation as follows:

{$$}
\frac{H_0}{2} = H_t
{/$$}

which is to say that we want to know at which point the current genetic variation, {$$}H_t{/$$}, is exactly half of that of the one we had initially, {$$}H_0{/$$}. Because we have established above the equation for {$$}H_t{/$$}, we can simply replace it and write

{$$}
\frac{H_0}{2} = (1 - \frac{1}{2N})^t H_0
{/$$}

Now all we have to do is solve this equation for *t*. First, divide by {$$}H_0{/$$} on both sides:

{$$}
\frac{1}{2} = (1 - \frac{1}{2N})^t
{/$$}

Now, using the natural logarithm, we can get *t* down from the exponent:

{$$}
ln(\frac{1}{2}) = ln(1 - \frac{1}{2N}) t
{/$$}

That’s as far as we would get with regular algebra - but we can use an approximation to simplify this equation even more. The approximation is the following:

{$$}
ln (1+x) ≅ x
{/$$}

and it works only when *x* is small. So in our case, we can say that 

{$$}
ln(1 - \frac{1}{2N}) ≅ \frac{-1}{2N}
{/$$}

which simplifies our equation above to 

{$$}
ln(\frac{1}{2}) ≅ \frac{-1}{2N} t
{/$$}

Let’s multiply both sides by *-2N*, and we’ll get

{$$}
-2N ln(\frac{1}{2}) ≅ t
{/$$}

You may remember that {$$}ln(\frac{1}{x}) = -ln(x){/$$}. We’ll make use of this and replace {$$}ln(\frac{1}{2}){/$$} with *-ln(2)*:

{$$}
-2N(-ln(2)) ≅ t
{/$$}

which simplifies to 

{$$}
t ≅ 2N ln(2)
{/$$}

Since *ln(2)* is roughly 0.7, the equation finally is

{$$}
t ≅ 1.4N
{/$$}

That’s a pretty simple result. What it says is that the time it takes to half the genetic variance in a population by drift is roughly *1.4N* generations. So for a population of size 100, it takes about 140 generations, for a population of size 10,000, it takes about 14,000 generations, and so on. As expected, drift reduces genetic variation faster in small populations. We already established this in the equation above. The new insight we gained here is how long this process takes, and the answer is, a rather long time. Recall from the previous chapter how fast genotype frequencies go into Hardy-Weinberg equilibrium: it just takes a single generation. In contrast, drift takes thousands of generations to just half the genetic variance in even relatively small populations. Drift, it seems, is a rather slow process.

Let’s briefly summarize what this simple mathematical exercise has revealed:

* Genetic drift acts to reduce genetic variation.
* The effect of drift is proportional to the population size: the smaller the population, the larger the effect of drift.
* Unless other forces counter it, genetic drift will reduce all genetic variation in every population.
* Drift is a slow process - it takes about *1.4N* generation to reduce the genetic variation of a population by half.

These are fundamental insights into the biology of populations. They are also in agreement with our stochastic simulations.

I hope to have shown you that both approaches - that of the computational simulation, and that of the mathematical analysis - are valuable and correct approaches to address a problem. They’re certainly the most powerful when they’re combined, but it’s important to note that each has its unique strengths. 

##Effective population size

In the remainder of this chapter, I want to introduce an important concept that should always be on your radar when talking about drift: that of effective population size. We have until now assume a finite population size of *N*. We have also assumed that the population size is constant overt time (i.e. *N* in every generation). Both the simulations and the mathematical equations were based on this assumption. In reality, the size of a population can fluctuate over generations, sometimes strongly so.

Another assumption we’ve made so far is that we did not differentiate between sexes, and simply assumed that individuals are *hermaphroditic*, which is to say that they are both male and female at the same time. That assumption is violated in many species, although hermaphroditism is widespread among plants, and certain groups of animals such as snails and slugs.

There are other assumptions that we may have violated, so the question naturally emerges whether the simplified Wright-Fisher model is useful at all. The answer to this question is a resounding yes. As you have seen, the simplifying assumptions of the Wright-Fisher model allowed us to derive many important effects of a finite population size. The problem is, each of the violated assumptions, when taken into account, may have different effects on evolutionary dynamics, so how are we going to be able to reconcile all these differences? How can we compare different populations that have different specific biological characteristics? What we need is a way for us to translate what we observe in nature into something we can all agree on when talking about models. That something is the *effective population size*. 

Before I go into the details, let’s make sure we all understand this idea. When you talk about the effect of drift in your population, your focus should be on the effective population size, not the actual size of your population (commonly referred to as the *census* population size). Another way of saying this is as follows: from the perspective of genetic drift, it is not really important how large your population is in reality, if you were to count up the individuals - what matters is *as how large a population it behaves if it were a Wright-Fisher population*.

This is useful for a number of reasons. I have already mentioned one of the reasons above, which is that it allows for much more sensible comparisons of populations when you are interested in the effect of genetic drift. The second reason is that the effective population size, as it turns out, is usually *smaller* than the census population size, for reasons we’ll go into in a minute. Thus, you would usually underestimate the force of genetic drift if you would make your calculations based on the actual number of individuals in your population.

Here is a way of showing this graphically:

// TODO figure

As you can see, the effect of genetic drift, which is due reduce genetic variation, follows the pattern that you would expect in a Wright-Fisher of population of size 100. In other words, the effective population size of this population is 100, whereas the census population size is 200. Or put differently, even though this population consists of 200 individuals, it behaves - from a genetic drift perspective - as if it consisted of 100 individuals in a Wright-Fisher population. And this brings us to the definition of the effective population size, which is *the size of an ideal (Wright-Fisher) population that shows the same decay of genetic variation as the actual population of interest*.

Now that we have established the concept of the effective population size, let’s take a look at why the effective population size is usually smaller than the census population size. There can be a number of reasons, but I will focus on two of the most important ones.

The first reason for a substantially reduced effective population size are *population bottlenecks*. Sometimes, populations that are usually largely stable in population size may experience a substantial reduction in population size. Perhaps the population has been infected by a very deadly pathogen, or predation pressure has been unusually high. In that case, even just one generation of a strong population size reduction can have a very strong effect on the effective population size.

Let’s look at a simple numerical example. Let’s assume that the size of a population is usually 1,000 individuals, However, for reasons unknown, the population size collapses to 100 in one generation, before going back up to 1,000 again in the following generation.

If we look at just three generations in a row, where the population sizes were 1000, 100, and 1000 individuals, respectively, we could be forgiven for thinking that the effective population size in these three generations would be the average, which is 700. It turns out that the effective population size is the *harmonic mean* of the population sizes, which is 250. This is quite remarkable - this single generation bottleneck has slashed the effective population size to a number that’s almost three times smaller than what you would have expected by simply calculating the average. To make an even more extreme sample, let’s say the population sizes in these three generations are 1000, 10, and 1000. The average population size over the three generations is 670, but the effective population size is 29.4!

Let’s have a quick look at the definition of the harmonic mean - for any set of numbers {$$}x_1{/$$}, {$$}x_2{/$$}, {$$}x_3{/$$} … {$$}x_n{/$$}, the harmonic mean is

{$$}
\frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + \frac{1}{x_3} + … + \frac{1}{x_n}}
{/$$}

Because this term is a reciprocal of a sum of reciprocal, small numbers will have very strong effects on the mean. For the numbers 1000, 10, and 1000, the mean is 

{$$}
\frac{3}{\frac{1}{1000} + \frac{1}{10} + \frac{1}{1000}} = \frac{3}{0.102}
{/$$}

The reason why the harmonic mean is so low (29.4) is because the denominator is rather large, and it itself is almost fully determined by the value {$$}\frac{1}{10}{/$$}. Even if you had population sizes of 1,000,000 before and after the bottleneck, the formula shows that they wouldn’t have mattered the slightest bit, because then the mean would simply be equal to {$$}\frac{3}{0.100002}{/$$}, which is still about 30. The other important thing that the formula shows is that even when you have longer time periods of large, stable population sizes, the one generation with population size 10 can still dominate the outcome, and the effective population size would still be much smaller than expected.

The important message here is obvious: because the effective population size is given by the harmonic mean, population bottlenecks, even if over very short periods of time, will dramatically reduce the effective population size.

Let’s take a look at this claim with code. 

First, we need to make a small adjustment in our drawing code. In the function `draw_line_chart()` that I previously asked you to copy and paste, find the string `"Population Size:"` (towards the end in the code) and replace it with `"Eff. Population Size:"`.

Next, if we want to display the effective population size, we need to calculate it. In order to do this, we need to keep track of the population sizes over time. To do this, I first define a new variable which will hold the population sizes over the course of a simulation:

~~~~~~~~
var population_sizes = [];
~~~~~~~~

This variable holds an empty array into which I’m going to add the population sizes. In order to do that, I will change my simulation function to

~~~~~~~~
function simulation(simulation_counter) {
    p = 0.5;
    for (var i = 0; i < generations; i = i + 1) {
      	population_sizes.push(N);
      	next_generation(data[simulation_counter]);
    }
}
~~~~~~~~

All I’m changing here is the line 

~~~~~~~~
population_sizes.push(N);
~~~~~~~~

which will add the population size `N` into the array. "But wait", I hear you say, "since `N` currently remains constant, we are always pushing the same value into the array!" That’s right, and we’ll change that in a little bit. For now I just want to make sure we have all the pieces set up.

Now that we have our array with all the population sizes over the course of a simulation, we need to write a function that can actually calculate the effective population size based on the values in the array. Here’s how we can do this:

~~~~~~~~
function effective_population_size(population_sizes) {
    var denominator = 0;
    for (var i = 0; i < population_sizes.length; i = i + 1) {
        denominator = denominator + (1 / population_sizes[i]);
    }
    return Math.round(population_sizes.length / denominator);
}
~~~~~~~~

This function takes an array as an argument, iterates through its elements, and calculates the harmonic mean of its values. In order to avoid rounding issues, we simply return the number rounded to the closest integer by using the method `Math.round()`.

Add the function somewhere in your code, and call it like so (after you’ve run the simulations):

~~~~~~~~
Ne = effective_population_size(population_sizes);
~~~~~~~~

Then, when calling the `draw_line_chart()` function, simply pass in `Ne`, instead of `N`:

~~~~~~~~
draw_line_chart(data,Ne,generations);
~~~~~~~~

Set the population size `N` to `1000`, and the number of generations to `100`:

~~~~~~~~
var N = 1000;
var generations = 100;
~~~~~~~~

Save the document and load it in the browser. You should see something like this: 




Which is exactly what we expected. Since `N` remains constant throughout the `100` generations, the effective population size will be `1000`.

Now let’s adapt the code further to allow for a change in population size. We are going to add a slight modification that allows us to introduce a bottleneck of `N = 10` every `10` generations.

First, let’s briefly do the math. If we did not know about the harmonic mean, we might have thought that the average population size would be a good predictor of the dynamics. The average population size of `90` generations at `N = 1000` and `10` generations at `N = 10` is `901`. If we run ten simulations, using our current code with `N = 901`, we get the following outcome: 
 
As expected, because of the reduced population size, the allele fluctuations are a bit more pronounced, but by and large the effect is small. 

On the other hand, I have just claimed above that the population will behave much more like a Wright-Fisher population of a size that corresponds to the effective population size, which in this case would be `92`. How does a Wright-Fisher population of size 92 behave? Let’s find out by setting `N` to `92`, and here is what we get: 

Obvisouly quite different - a substantial reduction in genetic variation, with some populations having lost all variation by generation `100` already.

Let me repeat this - if the claim about effective population size and harmonic mean is indeed correct, then the dynamics of a population with `N = 1000` that goes through `N = 10` bottlenecks every 10^th^ generation should be much more like a population that has a constant population size of `N = 92`, rather than `N = 901` (and as the two previous images have shown, the dynamics would be quite different).

Only one way to find out! Let’s go to the code and implement the actual bottlenecks.

First, just to be sure, set `N` back to `1000`.

Then, since we want each generation to be able to have a different population size, we will pass the population size as a parameter to the `next_generation()` function. To do that, we need to change the function slightly to read

~~~~~~~~
function next_generation(simulation_data, N) {
    var draws = 2 * N;
    var A1 = 0;
    var A2 = 0;
    for (var i = 0; i < draws; i = i + 1) {
        if (Math.random() <= p) {
            A1 = A1 + 1;
        }
        else {
            A2 = A2 + 1;
        }
    }
    p = A1/draws;
    simulation_data.push(p);
}
~~~~~~~~

All that has changed is the number of arguments - the function body has remained identical. How can this be? Before, when the function did not expect the argument `N` for the population size, `N` in the function body referred to the global variable `N`. Now that the function has an argument called `N`, `N` will be available in the function body as a local variable. Importantly, therefore, `N` will not refer to the global variable, but rather to the local variable that contains the value that was passed to the function as a parameter.

Now let’s implement the occasional bottleneck. Here’s how we will do it:

~~~~~~~~
function simulation(simulation_counter) {
    p = 0.5;
    var population_size;
    for (var i = 0; i < generations; i = i + 1) {
        if (i%10 == 9) {
            population_size = 10;
        }
        else {
            population_size = N;
        }
        population_sizes.push(population_size);
next_generation(data[simulation_counter],population_size);
    }
}
~~~~~~~~

Here’s what has changed. First, we define a local variable called `population_size`. We don’t assign it any value yet - that’s about to happen in the `for` loop. In the loop, I’ve added the following lines of code:

~~~~~~~~
if (i%10 == 9) {
    population_size = 10;
}
else {
    population_size = N;
}
~~~~~~~~

But wait - what is this `i%10 == 9` business? The `%` operator is called the *modulus* operator. It returns the integer remainder of a division.

Thus, 

`0 % 10`  equals `0`  
`1 % 10`  equals `1`  
`2 % 10`  equals `2`  
`3 % 10`  equals `3`  
`4 % 10`  equals `4`  
`5 % 10`  equals `5`  
`6 % 10`  equals `6`  
`7 % 10`  equals `7`  
`8 % 10`  equals `8`  
`9 % 10`  equals `9`  
`10 % 10`  equals `0`  
`11 % 10`  equals `1`  
etc.

In other words, with `i` starting at `0`, when `i%10` equals `9`, we are in the 10^th^, 20^th^, 30^th^ etc. generation. And whenever that is the case, we set the population size to `10`, otherwise we set it to `N`.

In the following two lines,

~~~~~~~~
population_sizes.push(population_size);
next_generation(data[simulation_counter],population_size);
~~~~~~~~

I simply changed `N` to `population_size`, because it is the variable `population_size` that holds the value of the population size in the current generation.

That’s it! Save the document and load it in a browser, and here’s what you’ll get:
 

Wow - that does look much more like the Wright-Fisher population at a constant `N = 92`! The effective population size is indeed `92` - substantially lower than the average in the census population size, which is `901`. You can clearly see when the population goes through the bottlenecks, but if you flatten out those jumps visually in your mind, you can see that the population behaves pretty much like a Wright-Fisher population at `N = 92`, rather than at `N = 901`.

To make an even more extreme case, set `N = 100000`, but keep the bottlenecks in the code. Your simulations will now of course run a bit slower (they take a few seconds on my laptop computer), but here’s what you will see when the they are done:
 

As you can see, the effective population size is still very small (`100`), despite the fact that this population has a size of `100000` during 90% of the time! You can also see that the dynamics are completely driven by the bottlenecks.

To sum up: changing census population sizes can have a strong effect on the effective population size, especially when the population goes though severe bottlenecks. The effect of bottlenecks is to reduce the effective population size, which will result in a much faster reduction in genetic variation.

The second reason for a substantially reduced effective population size is an *unbalanced sex ratio*. Recall that the Wright-Fisher model assumes hermaphroditic individuals. Because many species are not hermaphroditic, but rather have individuals with distinct sexes, the sex ratio can have a big impact on the effective population size. Imagine a population of 100 individuals, where 99 are female, and only one is male. The next generation will all have genes from the one male, and it is intuitively clear that this offspring generation will have a much lower genetic variance than it would have had, if the parent generation had had a more balanced sex ratio.

As it turns out, there is a simple formula to calculate the effective population size if we know the number of females, {$$}N_f{/$$}, and the number of males, {$$}N_m{/$$}, in a population ({$$}N = N_f + N_m{/$$}). The formula is 

{$$}
N_e = \frac{4N_mN_f}{N}
{/$$}

If the sex ratio is balanced such that {$$}N_m = N_f{/$$}, then it’s easy to see that the census population size and the effective population size are equal: because {$$}N_m = N_f{/$$}, it follows that {$$}N_m = N_f = \frac{N}{2}{/$$}, and so 

{$$}
\frac{4N_mN_f}{N}
{/$$}

becomes

{$$}
\frac{4\frac{N}{2}\frac{N}{2}}{N}
{/$$}

which is

{$$}
\frac{4\frac{N^2}{4}}{N}
{/$$}

which is 

{$$}
\frac{N^2}{N}
{/$$}

which is *N*, and thus {$$}N_e = N{/$$} in that case.

However, let’s look at an example where the ratio is very unequal. Unequal sex ratio may be due to sexual selection (where females prefer certain males over others, and vice versa), or due to artificial breeding (where a farmer has just a few males but many females), or due to other reasons. Let’s look at the case where we have a population of 10 male bulls and 90 female cows. Thus, {$$}N_f = 90{/$$}, and {$$}N_m = 10{/$$}. If you plug these numbers into the equation above, you get

{$$}
N_e = \frac{4 * 90 * 10}{100} = 36
{/$$}

Thus, the effective population size is only slightly more than a third of the actual census population size. 

We can get a good feeling for how strong this effect is going to be by plotting the ratio {$$}\frac{N_e}{N}{/$$} against the sex ratio. {$$}\frac{N_e}{N}{/$$} tells us how large the effective population size is, relative to the census population size. In the example above, the ratio would be {$$}\frac{36}{100} = 0.36{/$$}, which is quite low. On the other hand, our sex ratio was very distorted. Let’s plot the two quantities against each other.

First, we need a little math. I would like {$$}\frac{N_e}{N}{/$$} plotted as a function of the sex ratio. I will chose the ratio of {$$}\frac{N_m}{N}{/$$} as my sex ratio. So how can I transform this equation

{$$}
N_e = \frac{4N_mN_f}{N}
{/$$}

into one that expresses {$$}\frac{N_e}{N}{/$$} as a function of {$$}\frac{N_m}{N}{/$$}? A few easy steps! First, divide both sides by *N*:

{$$}
\frac{N_e}{N} = \frac{4N_mN_f}{N^2}
{/$$}

We can rewrite this as

{$$}
\frac{N_e}{N} = 4\frac{N_m}{N}\frac{N_f}{N}
{/$$}

and since {$$}\frac{N_f}{N} = 1 - \frac{N_m}{N}{/$$}, we can write

{$$}
\frac{N_e}{N} = 4(\frac{N_m}{N})(1-\frac{N_m}{N})
{/$$}

That's it: {$$}\frac{N_e}{N}{/$$} is now expressed as a function of {$$}\frac{N_m}{N}{/$$}. Now, let’s write the JavaScript code that calculates {$$}\frac{N_e}{N}{/$$} for values of the male proportion ranging from 0 to 1, in increments of 0.01:

~~~~~~~~
var data=[];
var x_max = 1;

for (var i = 0; i <= x_max + 0.005; i = i + 0.01) {
    var male_to_female_ratio = i;
    Ne_to_N_ratio = 4 * male_to_female_ratio * (1 - male_to_female_ratio);
    data.push(Ne_to_N_ratio);
}

draw_line_chart(data,x_max,"Proportion Males","Ne / N");
~~~~~~~~

As you can see, I’m implementing the equation above, and push the result into an array that we’ll hand over to the plotting function (which I will show you in a second). First, there are two subtleties in the `for` loop that I want to mention. First, I’m using the smaller than or equal operator, because I want to include the last value, `x_max` (which I’m setting to `1`). Second, I’m not actually writing 

~~~~~~~~
i <= x_max;
~~~~~~~~

but rather

~~~~~~~~
i <= x_max + 0.005;
~~~~~~~~

why is that? It’s because the rounding error would kick in again otherwise. Feel free to output the value of `i` to the console - you would see that after you’ve added the value `0.1` a hundred times to `0`, your final value will be `1.0000000000000007`, rather than `1`. To be sure that the for loop doesn’t end prematurely, I’m adding a small amount as an error margin to `1` when making the comparison.

Here’s the drawing function which you can again just copy and paste:

~~~~~~~~
function draw_line_chart(data, x_max, x_label, y_label) {
    var margin = {top: 20, right: 20, bottom: 50, left: 50},
            width = 700 - margin.left - margin.right,
            height = 400 - margin.top - margin.bottom;

    var x = d3.scale.linear()
            .domain([0, x_max])
            .range([0, width]);

    var y = d3.scale.linear()
            .range([height, 0]);

    var xAxis = d3.svg.axis()
            .scale(x)
            .orient("bottom");

    var yAxis = d3.svg.axis()
            .scale(y)
            .orient("left");

    var line = d3.svg.line()
            .x(function (d, i) {
              return x((i/(data.length-1)) * x_max);
            })
            .y(function (d) {
              return y(d);
            });

    var svg = d3.select("body").append("svg")
            .attr("width", width + margin.left + margin.right)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform", "translate(" + margin.left + "," + margin.top + ")");


    svg.append("g")
            .attr("class", "x axis")
            .attr("transform", "translate(0," + height + ")")
            .call(xAxis)
            .append("text")
            .attr("text-anchor", "middle")
            .attr("x", width / 2)
            .attr("y", 6)
            .attr("dy", "3em")
            .text(x_label);

    svg.append("g")
            .attr("class", "y axis")
            .call(yAxis)
            .append("text")
            .attr("transform", "rotate(-90)")
            .attr("x", -height / 2)
            .attr("dy", "-3.5em")
            .style("text-anchor", "middle").
            text(y_label);

    svg.selectAll("line.horizontalGridY").data(y.ticks(10)).enter()
            .append("line")
            .attr(
            {
              "x1" : 1,
              "x2" : width,
              "y1" : function(d){ return y(d);},
              "y2" : function(d){ return y(d);},
              "fill" : "none",
              "shape-rendering" : "crispEdges",
              "stroke" : "#eee",
              "stroke-width" : "1px"
            });

    svg.selectAll("line.horizontalGridX").data(x.ticks(10)).enter()
            .append("line")
            .attr(
            {
              "x1" : function(d,i){ return x((i/10) * x_max);},
              "x2" : function(d,i){ return x((i/10) * x_max);},
              "y1" : 1,
              "y2" : height,
              "fill" : "none",
              "shape-rendering" : "crispEdges",
              "stroke" : "#eee",
              "stroke-width" : "1px"
            });

    svg.append("path")
            .datum(data)
            .attr("class", "line")
            .attr("d", line);

  }
~~~~~~~~

And here is the outcome: 

As you can see, the effect is very strong when the sex ratio is strongly skewed. A quick check at a proportion of males at 10% shows that the graph correctly predicts that the effective population size will be 36% of the census population size, as our farm example above has shown.

Overall, the message here is that unless the sex ratio is severely skewed (less than 30% of one sex), the overall relative reduction in effective population size is small. However, beyond that value, the effect can be quite severe.

Boy, what a chapter! We’re done! Congratulations for following me all the way through here. Let’s briefly wrap up what we learned in this chapter.

* Finite population size introduces randomness into evolutionary dynamics (genetic drift).
* Genetic drift acts to reduce genetic variation.
* The effect of drift is proportional to the population size: the smaller the population, the larger the effect of drift.
* Unless other forces counter it, genetic drift will reduce all genetic variation in every population.
* Drift is a slow process - it takes about *1.4N* generation to reduce the genetic variation of a population by half.
* The effective population size is a key concept - it is the size of an ideal (Wright-Fisher) population that shows the same decay of genetic variation as the actual population of interest.
* The effective population size is often smaller than the actual population size, due to various reasons such as population size bottlenecks, or unequal sex ratios.
* We’ve also covered some key programming concepts such as arrays and conditional flows.

I hope you feel empowered to move on to the next chapter. You should be - you’ve mastered a lot of ground already. Think about where we started - at the very beginning, and yet at this point, you are already running stochastic simulations and plotting them in your browser, all the while learning key ideas about evolution.
