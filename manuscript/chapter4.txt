# 4. Mutation: The Power of Mistakes

In the previous chapter, we’ve learned a number of highly interesting aspects about evolution by genetic drift. One of the rather depressing consequences of drift, we learned, is the total elimination of genetic variation in the long run. Clearly, something else must be going on, for the world is not void of genetic variation. That something else is *mutation*.

Mutation is simply the random change in a genetic sequence. Most mutations occur when the genetic sequence (the DNA in nearly all living species) is replicated. The replication process, even though highly accurate, is not quite 100% error-free. Now and then, an error will occur, and the new copy will not be identical base for base to the original. 

In animals like ourselves, almost all mutations are so-called *somatic mutations*. That is, they occur in cells that are not passed on to future generations. These mutations are often contributing to cancer, but from an evolutionary perspective, they are a dead end. The mutations that do matter from an evolutionary perspective are the so-called *germ line mutations*. They occur in the germ line, i.e. the cells that will be passed on to future generations in the form of sperm or egg.

The ultimate effect of mutation is to increase genetic variation. Let’s write a short JavaScript program to illustrate that. Rather than using the two alleles model from earlier chapters, we’ll just use a simple DNA model. As you know, DNA is made up of four base pairs (Adenine, Guanine, Cytosine, and Thymine) that are usually abbreviated by their first letters A, G, C, and T.

First, let’s set up a few key parameters and data structures that we’ll be using:

~~~~~~~~
var sequences = [];
var original_sequence = [];
var number_of_sequences = 100;
var sequence_length = 20;
var number_of_generations = 100;
var mutation_rate = 0.0001; // per base and generation

var BASES = ['A','G','C','T'];
~~~~~~~~

We’ll be storing a number of sequences in the array `sequences` - each sequence will itself be an array of the characters `'A'`, `'G'`, `'C'` and `'T'`. We’ll store the original sequence in the array `original_sequence`. We would like to have `100` sequences in total (defined by `number_of_sequences`), and each sequence should consist of `20` base pairs (defined by `sequence_length`). Starting from an originally uniform population (i.e. no genetic variation - all sequences will be exact copies of the `original_sequence` initially), we’ll run `100` generations (defined by `number_of_generations`) during which we will have mutations occur with a probability of `0.0001` per base and generation (defined by `mutation_rate`). That is, in each generation, each base pair of each sequence has a `1` in `10,000` chance of “switching” to another base pair.

If you are wondering why the variable `BASES` is in capital letters: I’m simply following the established practice of using all capital letters in variables that are expected to be constant and never changing. JavaScript doesn’t have a built in mechanism to make a variable immutable, and thus making it clear to others that this variable shouldn't change by using all capital letters is considered the best practice.

Let’s first establish the code for generating the first generation. Here it is:

~~~~~~~~
function random_base(current_base) {
    var new_base;
    do {
        var index = Math.floor(Math.random() * 4);
        new_base = BASES[index];
    } while (new_base == current_base);
    return new_base;
}

function generate_original_sequence() {
    for (var i = 0; i < sequence_length; i = i + 1) {
        original_sequence.push(random_base(""));
    }
}

function generate_first_generation() {
    generate_original_sequence();
    for (var i = 0; i < number_of_sequences; i = i + 1) {
        sequences.push(original_sequence.slice());
    }
}

function print_sequences(title) {
    console.log(title);
    for (var i = 0; i < number_of_sequences; i = i + 1) {
        print_sequence(sequences[i]);
    }
    console.log('');
}

function print_sequence(sequence) {
    var sequence_string = "";
    for (var i = 0; i < sequence.length; i = i + 1) {
        sequence_string = sequence_string + sequence[i];
    }
    console.log(sequence_string);
}


generate_first_generation();
print_sequences("Generation 0");
~~~~~~~~

Even though this is quite a bit of code, you should be able to read through it and understand most of it. I’ll walk you through it in a second, but before we do that, take a look at the overall structure here. As you can see, the code is more or less packaged into functions that have a very specific task. At the end, we call two functions directly, and that’s it. This should make it much easier to read and understand the code. 

So rather than going from top to bottom of the code, I’ll start where the first function is called: `generate_first_generation()`. The first thing that the function does is call another function, `generate_original_sequence()`, so let’s go and take a look at that first. The function `generate_original_sequence()` simply contains a `for` loop that will add a random base to the `original_sequence` array. How does it get a random base? By calling the function `random_base()` with an empty string argument, so let’s go and take a look at that function.

The function `random_base()` should return a new base that is different from a given base. It should be different from a given base because we want to use this function later on when we mutate a DNA sequence. If we happen to get to the base `'A'`, say, and that function would return `'A'` also, then we wouldn’t have a mutation - the base is still `'A'`. This is why we added an argument to the function - so that we can essentially tell the function to give us a new base that is *different* from a given base.

We do this by using a `do while` loop, which is something you haven’t seen before. The only loop you’ve encountered so far was the `for` loop, which is very practical in general, although not for all uses. The `for` loop is a straightforward construct when you know how often you want to iterate. However, in this case, our requirements are a little different from that. What we want to express, in code, is the following: draw a random letter from the letters `'A'`, `'G'`, `'C'` and `'T'`, and continue to do that as long as the drawn letter is the same as the one I have given you. The `do while` loop is ideal for this kind of situation. It says “do something while a certain condition is met”. 

Its structure is simple:

~~~~~~~~
do {
    code to be iterated
} while (condition);
~~~~~~~~

One of the side effects of this construct is that the code to be iterated *is guaranteed to be executed at least once*. It may be executed many more times, as long as the condition is met, but it will be executed at least once. And this is exactly what we need: we need to draw at least one base pair, and if that base pair does not meet our condition, we will continue to draw a base pair until that condition is met. Let’s take a detailed look at how we do this:

~~~~~~~~
do {
    var index = Math.floor(Math.random() * 4);
    new_base = BASES[index];
} while (new_base == current_base);
~~~~~~~~

In other words, we draw a random integer between `0` and `3` (note that `Math.floor()` always rounds down to the next lower integer), which we will use as our index in the `BASES` array. We then store that selected base in the variable `new_base`, and compare it to the base that we passed to the function as the parameter `current_base`. If the two are identical, we’ll repeat the process until they’re not identical anymore.

Note that in the function `generate_original_sequence()`, we pass an empty string as a parameter to the function `random_base()` - you hopefully now understand why: the randomly drawn base will be compared to an empty string, and thus the function will always return the first randomly drawn base, which is what we want when we generate the first random sequence.

After the function `generate_original_sequence()` has finished executing, we’re going back to the function `generate_first_generation()`, where we find another `for` loop. This time, the loop makes copies of the original sequence and saves those copies in the `sequences` array. The copying is done using the `slice()` method, which copies an array (another handy built-in method of the array object that you might want to use in the future).

Once that’s done, we’re done with the function `generate_first_generation()`, and can go to the next function we’re calling, which is `print_sequences()`. This function, and the function it calls, is very simple and has no new concepts, so I will trust that you understand what’s happening here. It is simply printing all the sequences into the console.

If you save this code in a new file and load it in a browser, you should see the following in your console (depending on the browser): 

{width=50%}
![](images/ch_4_one_sequence.png)

The console window in my browser (Chrome) is being nice here, and rather than printing the same output a hundred times, it is printing it once, and letting me know that it was asked to print the same output `100` times. If you reload, you should see the same thing, but with an other sequence. 

The code seems to be working well. You now have a uniform population of `100` DNA strings, and no variation whatsoever. Now let’s introduce mutation. 

I’m going to add one method:

~~~~~~~~
function run_generations() {
    for (var i = 0; i < number_of_generations; i = i + 1) {
        for (var ii = 0; ii < sequences.length; ii = ii + 1) {
            for (var iii = 0; iii < sequences[ii].length; iii = iii + 1) {
                if (Math.random() < mutation_rate) {
                    sequences[ii][iii] = random_base(sequences[ii][iii]);
                }
            }
        }
    }
}
~~~~~~~~

and also add these two method calls after `print_sequences("Generation 0")`: 

~~~~~~~~
run_generations();
print_sequences("After 100 generations");
~~~~~~~~

Granted, the function `run_generations()` looks a little intimidating. But we’ll walk through it line by line. Notice that there is no new concept here - just loops and arrays. The only unusual thing is that we have three loops at once, and they are *nested*. The first loop iterates over the generation counter. The second loop iterates over all sequences. Because the two loops are nested, we can say that in every generation, we loop over all sequences. But it doesn’t stop there - we have a third loop which iterates over all bases. In other words: in every generation (first loop), we go through each sequence (second loop) and iterate over each base (third loop).

Notice that each loop has a different counter variable. I use `i`, `ii`, `iii` etc. Other people use `i`, `j`, `k`, etc. I prefer my style because it instantly tells me which nesting level I’m at, but feel free to use whichever variable names you prefer.

Once we are in the third loop that iterates over each single base pair in a sequence, we can trigger a mutation event with probability `0.0001`. As before, we are implementing this stochastic event by using the built-in random number generator `Math.random()`. Let’s take a look at how we do this:

~~~~~~~~
if (Math.random() < mutation_rate) {
    sequences[ii][iii] = random_base(sequences[ii][iii]);
}
~~~~~~~~

The expression `sequences[ii][iii]` is accessing an element from a two-dimensional array. If this doesn’t ring a bell, be sure to re-read the discussion about arrays in the previous chapter. The array `sequences` contains all sequences, which are themselves arrays of base pairs. Thus, `sequences[ii]` is itself an array (an array of bases), and the expression `sequences[ii][iii]` means the `iii`^th^ element (the base pair) in the `ii`^th^ sequence, where `iii` and `ii` are the counters of the two innermost `for` loops. 

Now, the complete code is as follows:

~~~~~~~~
var sequences = [];
var original_sequence = [];
var number_of_sequences = 100;
var sequence_length = 20;
var number_of_generations = 100;
var mutation_rate = 0.0001; // per base and generation

var BASES = ['A','G','C','T'];

function random_base(current_base) {
    var new_base;
    do {
        var index = Math.floor(Math.random() * 4);
        new_base = BASES[index];
    } while (new_base == current_base);
    return new_base;
}

function print_sequence(sequence) {
    var sequence_string = "";
    for (var i = 0; i < sequence.length; i = i + 1) {
        sequence_string = sequence_string + sequence[i];
    }
    console.log(sequence_string);
}

function generate_original_sequence() {
    for (var i = 0; i < sequence_length; i = i + 1) {
        original_sequence.push(random_base(""));
    }
}

function generate_first_generation() {
    generate_original_sequence();
    for (var i = 0; i < number_of_sequences; i = i + 1) {
        sequences.push(original_sequence.slice());
    }
}

function run_generations() {
    for (var i = 0; i < number_of_generations; i = i + 1) {
        for (var ii = 0; ii < sequences.length; ii = ii + 1) {
            for (var iii = 0; iii < sequences[ii].length; iii = iii + 1) {
                if (Math.random() < mutation_rate) {
                    sequences[ii][iii] = random_base(sequences[ii][iii]);
                }
            }
        }
    }
}

function print_sequences(title) {
    console.log(title);
    for (var i = 0; i < number_of_sequences; i = i + 1) {
        print_sequence(sequences[i]);
    }
    console.log('\n');
}

generate_first_generation();
print_sequences("Generation 0");
run_generations();
print_sequences("After 100 generations");
~~~~~~~~

If you save the file and load in the computer, you’ll see something like this:
 
{width=50%}
![](images/ch_4_many_sequences.png)

In the interest of space, I’m only showing a fraction of what you’ll see, but the point is as follows: mutation has introduced a substantial amount of genetic variation. In an example run on my machine, there are now 34 unique genetic sequences, and the population is dominated by 4 sequences that each occur more than 10 times. Of course, this will be different every time you re-run the simulation, but it’s nevertheless remarkable. We are often tempted to think that mutations rates as low as 1 in 10,000 don’t have a strong effect, simply because 1 in 10,000 is an unlikely event. Nevertheless, if you have lots of base pairs in lots of individuals over many generations, even much lower mutation rates will produce a substantial amount of genetic variation. 

Feel free to play around with the value of the mutation rate. If you increase the mutation rate by a factor of ten, to `0.001`, you’ll notice that you get even more variation: almost all sequences will be unique. Conversely, if you decrease the mutation rate by a factor of ten, to `0.00001`, you’ll notice that you get very little variation, and sometimes even none at all in a single simulation run.

Mutation rates in nature are quite variable. In humans, the estimates are somewhere around 10^-10^ per base pair and replication, and 10^-8^ per base pair and generation - that is, around 1 in 10 billion per base pair and replication, and 1 in 100 million per base pair and generation (human germ cells go through many replications per single human generation). At the lower end, some viruses have mutation rates as low as 1 in 1,000 per base pair and replication. It’s important to be careful about what exactly the mutation rate denotes. Mutation rates are always per some unit, and per some time interval. They can be per base pair, per gene / allele, or per genome (the totality of all base pairs). They can be per replication or per generation. 

Having established that mutations can generate genetic variation, we should notice something very interesting: that the consequences of mutation are the exact opposite of the consequences of drift. This is the central idea in this chapter, and one of the central ideas in evolution in general. Mutation creates genetic variation; drift eliminates it. 

We’ll have plenty to code later, but in this chapter I would first like to establish the mathematical foundations of this battle of forces, mutation vs. drift. We’ll use the same type of math that we used in the previous chapters, and I’ll be again very careful to walk you slowly through each step.

Let’s start again at the beginning. Last time, when were just looking at drift, we started with the observation that

{$$}
G’ = \frac{1}{2N} + (1-\frac{1}{2N})G
{/$$}

Let’s recall what that equation means. It simply means that the probability that two randomly picked alleles in the next generation will be the same (*G’*) depends on that very probability in the current generation (*G*) and the population size *N*. From this equation, we derived all the other insights about genetic drift in the previous chapter. Let us just revisit very briefly how we obtained this initial equation.

How can two randomly picked alleles be the same type of allele? There are two possibilities. Either, they happen to be descendants of the same parental allele, which occurs with probability {$$}\frac{1}{2N}{/$$}. Or, they are not descendants of the same parental allele (which happens with probability {$$}1-\frac{1}{2N}{/$$}), but their parental alleles are nonetheless of the same type (which is the definition of *G*). Notice what is missing here? Mutation! We have not considered the effect of mutation yet. The two alleles could in fact be different, simply because of a mutation that occurred during the copying process. So in order for the two alleles to be identical, it must be that no mutation occurred - otherwise they would be different (we’re ignoring here the astronomically small chance that they both mutated to the same new type). What is the probability that no mutation occurs in either of the two alleles?

If we define the mutation rate per allele per generation as *μ*, then the probability that no mutation occurs in either of the two alleles is *(1-μ)^2^*. Why is that? The probability the no mutation occurs in the first allele is *1-μ*. Equally, the probability that no mutation occurs in the second allele is also *1-μ*. Both of these non-events need to occur, and therefore we need to multiply their probabilities. You can verify this by thinking about throwing dices. The probability that you throw a 6 with one dice is {$$}\frac{1}{6}{/$$}. The probability that you don’t throw a 6 is {$$}1 -\frac{1}{6} = \frac{5}{6}{/$$}. Now if you throw two dice at the same time, what would be the probability that none of them would show a 6? The first dice must not show a 6 (probability {$$}\frac{5}{6}{/$$}) and the second dice must not show a 6 (probability {$$}\frac{5}{6}{/$$}), and therefore the total probability of not throwing a 6 with two dice is {$$}\frac{5}{6} * \frac{5}{6} = \frac{25}{36}{/$$} which is roughly 70%.

Going back to alleles, we can adjust our equation to take into account that no mutation happened in either two alleles, by writing

{$$}
G’ = (1-\mu )^2 (\frac{1}{2N} + (1-\frac{1}{2N})G)
{/$$}

We simply multiple the entire term by the probability that no mutation occurs in any of the two alleles, which is *(1-μ)^2^*. Let’s expand the mutation term, and we get 

{$$}
G’ = (1 - 2\mu  + \mu ^2) (\frac{1}{2N} + (1-\frac{1}{2N})G)
{/$$}

This can be simplified if we make the reasonable assumption that the mutation rate *μ* is small, and therefore, the square mutation rate will be extremely small to the point where we can ignore it in an addition. That is, we apply the approximation 

{$$}
1 - 2\mu  + \mu ^2 \approx 1 - 2\mu
{/$$}

when *μ* is small, giving us 

{$$}
G’ \approx (1 - 2\mu ) (\frac{1}{2N} + (1-\frac{1}{2N})G)
{/$$}

Let’s expand this further to

{$$}
G’ \approx \frac{1}{2N} + (1-\frac{1}{2N})G - \frac{2\mu }{2N} - 2\mu (1-\frac{1}{2N})G
{/$$}

The last term can be expanded a little further:

{$$}
G’ \approx \frac{1}{2N} + (1-\frac{1}{2N})G - \frac{2\mu }{2N} - 2\mu G + \frac{2\mu G}{2N}
{/$$}

The next simplifying approximation is similar to the previous one. As you can see, the right hand side is now an addition of terms, and some of the terms are so small, relatively to the others, that we can effectively ignore them. Which terms are those? It’s the terms that have both *μ* in the numerator, and *N* in the denominator. These terms are small because *μ* is small, and is made even smaller by the division by *N*. If we ignore these very small terms, we’ll get

{$$}
G’ \approx \frac{1}{2N} + (1-\frac{1}{2N})G - 2\mu G
{/$$}

Before we move on, let’s bring *H* back into the scene. Recall that *H* is simply the opposite of *G*, and thus *1 - G*, and thus *G = 1 - H*. Starting from 

{$$}
H’ = 1-G’
{/$$}

we can now insert the previous term for *G'*: 

{$$}
H’ \approx 1-(\frac{1}{2N} + (1-\frac{1}{2N})G - 2\mu G)
{/$$}

and replace all occurrences of *G* with *1 - H* to obtain

{$$}
H’ \approx 1- \frac{1}{2N} - (1-\frac{1}{2N})(1-H) + 2\mu (1-H)
{/$$}

If we fullly expand this equation, we’ll get

{$$}
H’ \approx 1 - \frac{1}{2N} - 1 + \frac{1}{2N} + H - \frac{H}{2N} + 2\mu  - 2\mu H
{/$$}

Since the first four terms of the rights side of the equation cancel each other, we get 

{$$}
H’ \approx H - \frac{H}{2N} + 2\mu  - 2\mu H
{/$$}

Let’s pause here for a moment. Let’s recall what we are trying to do there. We’ve established above that mutation and drift are two opposing forces - one increases genetic variation, the other decreases it. Since we have now established a value for *H’*, we can take a look at the difference in *H* in each generation, which we denote by *ΔH*:

{$$}
\Delta H = H’ - H
{/$$}

Thus *ΔH* is simply the change in genetic variation per generation. By replacing the above established value for *H’*, we get 

{$$}
\Delta H = H - \frac{H}{2N} + 2\mu  - 2\mu H - H
{/$$}

Because the *H* is canceling itself, we can simplify this to

{$$}
\Delta H = -\frac{H}{2N} + 2\mu  - 2\mu H
{/$$}

which can be rewritten as 

{$$}
\Delta H = -(\frac{1}{2N})H + 2\mu (1-H)
{/$$}

Note that this equation is a sum of a negative term and a positive term. Indeed, this is quite a beautiful equation: the negative term, {$$}-(\frac{1}{2N})H{/$$}, denotes the loss of genetic variation due to drift, while the positive term, *2μ(1 - H)*, denotes the gain of genetic variation due to mutation.

From this simple equation, which captures both processes elegantly, we can deduce another interesting observation. First, it confirms what we already know, which is that the loss of genetic variation depends on the population size *N*, in such a way that the loss is larger if *N* is smaller. But second, it also shows quite clearly that the gain of genetic variation depends on the mutation rate *μ*, such that the gain is higher if the mutation rate is higher, as we would expect.

Now, there is going to be a point where in each generation, the increase through mutation will be exactly offset by the decrease through genetic drift, i.e. *ΔH* will be zero. This is an an interesting point, because it represents an equilibrium: the genetic variation will neither go up nor go down, but simply remain the same forever. What stable genetic variation can we expect?

We’ve just established that

{$$}
\Delta H = -(\frac{1}{2N})H + 2\mu (1-H)
{/$$}

The equilibrium will be reached when *ΔH* is zero, i.e.

{$$}
-(\frac{1}{2N})H + 2\mu (1-H) = 0
{/$$}

If we bring the first term on the right hand side of the equation, we get

{$$}
2\mu (1-H) = (\frac{1}{2N})H
{/$$}

Let’s expand the left side of the equation:

{$$}
2\mu  - 2\mu H = (\frac{1}{2N})H
{/$$}

If we add *2μH* to both sides, we’ll get

{$$}
2\mu  = (\frac{1}{2N})H + 2\mu H
{/$$}

Factoring out *H* will yield

{$$}
2\mu  = H(\frac{1}{2N} + 2\mu )
{/$$}

Dividing both sides by {$$}(\frac{1}{2N} + 2\mu ){/$$} gives us *H*

{$$}
\frac{2\mu }{(\frac{1}{2N} + 2\mu )} = H
{/$$}

This looks a little unwieldy with a double fraction. Let’s simplify this by multiplying the term by {$$}\frac{2N}{2N}{/$$}, and we’ll get 

{$$}
H = \frac{4N\mu }{(1 + 4N\mu )}
{/$$}

This is much nicer indeed. It is also a rather profound result. It tells us how much genetic variation we should expect in a population subject to drift and mutation. 

Let’s plot this relationship between *H* and the term *4Nμ*. We can do this fairly simply by filling up a `data` array:

~~~~~~~~
var data=[];
var x_max = 10;

for (var i = 0; i <= x_max+0.005; i = i + 0.01) {
    data.push(i/(1+i));
}

draw_line_chart(data,"4N\u03BC","H",[],x_max);
~~~~~~~~

and then pass that to the `draw_line_chart()` function that we’ve used above (note that `\u03BC` is simply the unicode character for *μ*). Here’s what you’ll see:
 
{width=90%}
![](images/ch_4_H_vs_4nu.png)

As you can see, *H* is quite sensitive in the 0 - 1 range for *4Nμ*, and thereafter starts to asymptotically approach 1. The way to think about this as follows. If *4Nμ* is small (i.e. less than 1), the population will have little genetic variation at equilibrium. *4Nμ* can be small if either *N* is small, in which case the variance-reducing effect of drift would be strong, and / or the mutation rate *μ* can be low, which generates little genetic variation. On the other hand, if *4Nμ* is large (i.e. larger than 1), the population will have a lot of genetic variation at equilibrium. *4Nμ* can be large if either *N* is large, in which case there would be very little reduction of genetic variation by drift, and / or the mutation rate *μ* can be high, which continuously generates a lot of genetic variation.

The overall take-home message is the following - just as genetic drift reduces genetic variation, mutation increases variation. Over time, the two processes will reach an equilibrium which is given by both the population size *N*, and the mutation rate *μ*.

##The Fixation of Mutations

Let’s move on to another interesting dynamic. We know that mutation introduces new genetic variants. But what does the future of such a mutation look like? In the beginning, the mutation exists as a single copy. Recall that in our world, everything is selectively neutral - we still assume no form of selection whatsoever. It’s quite possible that such a rare mutation will be lost, due to random sampling, in the next generation already. It might stay around for a few generations, even manage to make a few copies of itself, and then disappear from the population. Or, it might actually take over the population and go to fixation!

You may think “yeah ok, but that’s probably rare” and you would be right, as we’ll see in a minute. Nevertheless, consider this: every allele that you see in a population, even the most common alleles, once started as a single copy. Granted, some of them might have been very beneficial to its bearer from the outside, which would have made its ride to domination over time a little easier. Nevertheless, even neutral alleles can go from a single copy to fixation in a population. The question is, how likely is that?

It turns out that the answer is very simple. The probability that a neutral allele will go to fixation is - drum roll... *its frequency*. Simple as that! If an allele has a frequency of 50%, the probability that it will go to fixation is 50%. If the allele has a frequency of 10%, the probability that it will go to fixation is 10%, and so on. What is the frequency of a single allele in a populate of size *N*? Since we’re talking about diploid individuals, each individual has two copies of each gene, and thus there are *2N* alleles, as we know. Thus, the frequency of the initial mutant allele is {$$}\frac{1}{2N}{/$$}, which is also the probability that it will go to fixation.

Let’s use some of the code that we already wrote to verify that this claim is actually true. In the previous chapter, all stochastic simulations that we ran started at `p = 0.5`, which meant that both alleles, A1 and A2, had a frequency of 50%. We can easily modify our code to ensure that `p` equals {$$}\frac{1}{2N}{/$$} - thus assuming that A1 is the mutant allele which has just come into existence in a population previously completely composed of A2 alleles. We will then run many, many simulations that we will stop when one of the two alleles, A1 or A2, has gone to fixation, and we’ll count the number of times that A1 has gone to fixation (i.e. where `p==1`). That number, divided by the total number of simulations, will be the probability of fixation. 

I invite you to try and write this code on your own. Don’t worry if you don’t manage to do it, I don’t expect you to. But it’s important to get your feet wet early - programming is all about practice. Feel free to copy from what we developed in the previous chapter. 

I’m going to go ahead and show you how I would do it. The code is as follows: 

~~~~~~~~
var N = 100;
var p;
var simulations = 10000;
var fixations_of_mutant = 0;

function next_generation() {
    var draws = 2 * N;
    var A1 = 0;
    var A2 = 0;
    for (var i = 0; i < draws; i = i + 1) {
        if (Math.random() <= p) {
            A1 = A1 + 1;
        }
        else {
            A2 = A2 + 1;
        }
    }
    p = A1/draws;
}

function run_until_fixation() {
    p = 1 / (2*N);
    do {
        next_generation();
    } while (p > 0 && p < 1);
    if (p == 1) {
        fixations_of_mutant = fixations_of_mutant + 1;
    }
}

for (var i = 0; i < simulations; i = i + 1) {
    run_until_fixation();
}
console.log(fixations_of_mutant / simulations);
~~~~~~~~

You should now be in good shape understanding almost everything this code does. The function `next_generation()` is identical to the one we used in the previous chapter. I’ve defined an additional function called `run_until_fixation()`, that runs a simulation until one of the two alleles has become fixed. Its code is relatively simple - first, we (re)set `p` to `1 / (2*N)`. Forgetting to do this is a typical beginner’s mistake. If you don’t reset `p` for every simulation run, only the first simulation run will be meaningful - all the following simulation runs would start with `p` either being 0 or 1, depending on the outcome of the very first simulation. Next, we call the `next_generation()` function however often we need to, as long as `p` is neither `0` nor `1`. But what’s this strange-looking operator `&&` doing there?

You’ve encountered the `do while` loop before. It says “do something while a certain condition is true”. However, so far in this book, we have only ever encountered simple conditions, where we tested for either a boolean value to be true, or where we tested for a number to be smaller, equal to, or greater than a certain value. But sometimes, our conditions need to be a bit more complex. For example, we may require that multiple expressions are true. Or, we may require that at least one of multiple expressions is true. This is where so-called *logical operators* come in handy.

The most frequent logical operators in JavaScript are 

~~~~~~~~
&& // the logical and
|| // the logical or
!  // the logical not
~~~~~~~~

The first, `&&`, is the *logical and*. If you need two expressions to be true at the same time, you should use this operator. For example, the condition 

~~~~~~~~
(p > 0 && p < 1)
~~~~~~~~

is true if `p` is greater than `0` *and* `p` is smaller than `1`.

The second operator is the *logical or*. If you have multiple expressions and you need at least one of them to be true, you should use this operator like so, for example:

~~~~~~~~
(p == 0 || p == 1)
~~~~~~~~

Finally, there is the *logical not* operator. It flips a boolean value on its head, like this

~~~~~~~~
(!something)
~~~~~~~~

If `something` is true, this condition is false. If `something` is false, this condition is true.

Going back to the code example, we run the generations while `p` is greater than `0` and smaller than `1` - which means that neither of the two alleles has gone to fixation yet. Once A1 goes to fixation - at which point `p` will be `1` - or A2 goes to fixation - at which point `p` will be 0 - the loop stops. If A1 has fixed, we’ll record that by increasing the counter `fixations_of_mutant` by `1`.

In the last loop in the code, we simply repeat this process a number of times (defined by the variable `simulations`) and then calculate and log the fraction of simulation runs in which `p` has gone from {$$}\frac{1}{2N}{/$$} to 1.

If you run this with the values that I’m using (i.e. `N = 100` and `10000` simulations), your results should look like this (reload the page a couple of times to get multiple values):

~~~~~~~~
0.0054
0.0053
0.0049
0.0045
0.0049
~~~~~~~~

etc.

Does this fit our prediction? Indeed it does. With `N=100`, {$$}\frac{1}{2N}{/$$} equals 0.005, and that’s exactly what we get here. 

Now go to the line in the code where you (re)set `p` to `1 / (2*N)`, and set it to, say, `0.1`, and run the simulation again. Your results should now look something like this:

~~~~~~~~
0.0976
0.103
0.1009
0.969
0.1038
~~~~~~~~

etc.

Again, the prediction is verified - in 10% of all simulations, the allele went from its initial frequency of 10% to 100% - i.e. the probability of the allele at `p = 0.1` to go to fixation is indeed 0.1. 

By the way, you may have noticed that the simulations that started at `p = 0.1` were slower than the simulations that started at `p = 1 / (2*N)`. Why is that? Recall that we only run a simulation until `p` becomes `0` or `1`. When we start the simulation at `p = 1 / (2*N)`, it will generally go to `0` very rapidly - there is only one copy of the mutation in the population, and it will often be lost immediately, terminating the simulation. On the other hand, if we start the simulation at `p = 0.1`, then `p` will also go to `0` in most of the simulations (in 90%, as you now know), but getting there takes a little longer because you are starting at a much larger value for `p` than just `1 / (2*N)`.

Ok, so we have established that the probability of a new mutation going to fixation is {$$}\frac{1}{2N}{/$$}. How many mutations are there per generation? Let’s see - we have *2N* alleles in the population, and the mutation rate per allele per generation is *μ*. Thus, there are *2Nμ* new mutants per generation. And as we now know, each of those mutations will go to fixation with probability {$$}\frac{1}{2N}{/$$}. Thus, each generation, *2Nμ* times {$$}\frac{1}{2N}{/$$} mutations will go to fixation, which is *μ*.

Let’s think about what this means. Let’s assume *μ* is 0.0001. Thus, each generation, 0.0001 mutations will go to fixation. Per se, that is a weird statement. A mutation either goes to fixation or it doesn’t. What we should rather say is that the probability that a mutation will go to fixation in a given generation is 0.0001. In the next generation, it is 0.0001 as well. And so on. In other words, every {$$}\frac{1}{0.0001} = 10,000{/$$} generations on average, a new mutation will become fixed in the population. Population geneticists refer to this idea as the replacement rate. In our case, we would say the replacement rate is 0.0001 per generation.

But have you noticed what happened here? The replacement rate corresponds exactly to the mutation rate *μ*! But where has the population size, *N*, gone? It turns out that the replacement rate is independent of the population size. Recall where we “lost” *N*: we said the replacement rate is the number of mutations per generation (which is *2Nμ*), times the probability of fixation of each of this mutations (which is {$$}\frac{1}{2N}{/$$}):

{$$}\text{replacement rate} = 2N\mu  * \frac{1}{2N} = \mu {/$$}

This a remarkable result. Most of us would have guessed that population size must play a role. You may have thought that in large populations, there would be more replacement than in small populations, because there are simply more mutations. On the other hand, your intuition may have told you - correctly - that it’s harder for mutations to go to fixation in larger populations, which may have led you to believe that the replacement rate would be lower. But as it turns out, the two effects - more mutations, but it’s harder for them to make it all the way to fixation - are both dependent on *N* in such a way that *N* falls out of the equation.

We now have a good feeling for how often a new mutant allele fixes in a population. But *how long* does it take for a successful mutant allele to do that, on average? Before I give you the answer to that, I would like you to adapt the code above to find out. You already have more or less everything in place, and all you need to do is to keep track of the number of generations in those runs where `p` actually goes to `1`.

Tried it? Ok, here is how I would do it:

~~~~~~~~
var N = 100;
var p;
var simulations = 100000;
var fixations_of_mutant = 0;
var total_generations = 0;

function next_generation() {
    var draws = 2 * N;
    var A1 = 0;
    var A2 = 0;
    for (var i = 0; i < draws; i = i + 1) {
        if (Math.random() <= p) {
            A1 = A1 + 1;
        }
        else {
            A2 = A2 + 1;
        }
    }
    p = A1/draws;
}

function run_until_fixation() {
    p = 1 / (2*N);
    var generations = 0;
    do {
        next_generation();
        generations = generations + 1;
    } while (p > 0 && p < 1);
    if (p==1) {
        fixations_of_mutant = fixations_of_mutant + 1;
        total_generations = total_generations + generations;
    }
}

for (var i = 0; i < simulations; i = i + 1) {
    run_until_fixation();
}
console.log(total_generations / fixations_of_mutant);
~~~~~~~~

As you can see, this code is almost identical to the one we used above. We’ve simply added a new global variable called `total_generations` that keeps track of the generations spent in those simulations where `p` goes to `1`. However, since we don’t know from the outset which simulation will actually result in `p==1`, we will have to keep track of the generations in each simulation. To do that, we introduce the local variable `generations` in the function `run_until_fixation()`, and we increase it by `1` in every generation. If the simulation ends up resulting in `p ==1`, we will add that generation count to the global variable `total_generations`. Thus, at the end of all simulations runs, `total_generations` will be the sum of all generations from all simulations where `p` went to `1`, and all that is left to do is to divide that number by the number of those successful fixation simulations, `fixations_of_mutant`.

By the way, the other change I’ve made is to increase the number of simulations from `10000` to `100000` - the simulations still run within a few seconds on my laptop, but you may have to adjust these numbers if it’s too slow for you.

If you run this a number of times, what are your results? With the sets of parameters shown in the code above, I get the following results (yours will vary, of course):

~~~~~~~~
405.17102615694165 
401.9110671936759
401.19109461966605 
392.43110236220474  
396.681640625 
~~~~~~~~

See the pattern? The answer is *4N* - if a mutant allele manages to go to fixation in a population of size *N*, it will take *4N* generations to do so on average.

And that concludes this chapter on mutation. We learned a number of really interesting things. The main points were the following:

* Mutation is the source of genetic variation. 
* Mutation acts to increase genetic variation, while drift acts to reduce it. When the effect of drift is exactly offset by the effect of mutation, the genetic variation in a population will remain in equilibrium.
* If a population is small, and / or has a low mutation rate, the genetic variation at equilibrium will be low. Conversely, if a population is large, and / or has a high mutation rate, the genetic variation at equilibrium will be high.
* The chance that a new mutation will go to fixation is {$$}\frac{1}{2N}{/$$}, its frequency. If it does go to fixation, the process will take *4N* generations on average.
* The replacement rate is given by the mutation rate, and does not depend on the population size.
* We have also covered a few new JavaScript concepts and methods: the `do while` loop, `Math.floor()`, and logical operators.

In this chapter, we have written quite a bit of code, and we have developed great intuitions about mutation and drift with simple algebra. However, we haven’t really had too much fun this time with visualizations. But you’ll be more than compensated for that in the next chapter, when we are going to create spatial, individual-based models. When I teach this material in a class, this is usually when students are most excited about the resulting animations. 

Ready for the next step? Let’s roll!

